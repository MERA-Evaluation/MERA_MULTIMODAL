# WEIRD


## Описание задачи

WEIRD – это расширенная версия подзадачи бинарной классификации оригинального английского бенчмарка [WHOOPS!](https://whoops-benchmark.github.io/). Датасет оценивает, способна ли мультимодальная модель обнаруживать нарушения здравого смысла в изображениях. Здесь нарушение здравого смысла – это ситуации, противоречащие типичным нормам реальности. Например, пингвины не могут летать, дети не водят автомобили, посетители не накладывают еду официантам, и так далее. В датасете поровну "странных" и обычных изображений.

Тестируемые навыки моделей: Weirdness understanding, Common everyday knowledge, Physical property understanding, Object function understanding, Identity & emotion understanding

Авторы: Елисей Рыков, Василий Коновалов, Александр Панченко


## Мотивация

Датасет ориентирован на проверку навыков оценки реальности изображения и подходит для оценки любых AI-моделей, способных анализировать изображение. Ключевая оцениваемая способность – это анализ визуальной информации изображения и сопоставление ее с привычными нормами реальности. Метрика оценки качества – accuracy. Так как датасет оценивает фундментальные способности модели оценивать "реальность", он будет интересен всем исследователям как один из базовых этапов проверки модели.


## Описание датасета

### Поля данных

Каждый вопрос в датасете содержит следующие поля:

- `instruction` [str] — Промпт-инструкция для модели, содержащая шаблон для вставки элементов вопроса.
- `inputs` — Вводные данные, формирующие задание для модели.
    - `image` [str] — Путь к файлу с изображением, к которому относится вопрос.
    - `question` [str] — Текст вопроса.
    - `option_a` [str] — Вариант ответа A.
    - `option_b` [str] — Вариант ответа B.
- `outputs` [str] — Правильный ответ на вопрос.
- `meta` — Метаданные, относящиеся к тестовому примеру, но не используемые в вопросе (скрытые от тестируемой модели).
    - `id` [int] — Номер-идентификатор вопроса в датасете.
    - `categories` — Категории признаков, характеризующих тестовый пример.
        - `commonsense_violation_subgroup` [str] — Подгруппа нарушения норм реальности, сгенерированная синтетически;
        - `commonsense_violation_group` [str] — Общая группа нарушения норм реальности, полученная вручную путем объединением подгрупп;
    - `pair_id` [str] — Номер пары изображений;
    - `image` — Метаданные, относящиеся к изображению.
        - `synt_source` [list] — Модели, используемые для генерации вопроса;
        - `source` [str] — Источник изображения


### Пример данных

```json
{
    "instruction": "Внимательно изучите предложенное изображение.\n<image>.\n Вопрос: {question}. Странные изображения противоречат здравому смыслу, в то время как нормальные ему соответствуют. В качестве ответа укажите только букву A или B без дополнительных рассуждений.\n\nA. {option_a}\nB. {option_b}\nОтвет:",
    "inputs": {
        "image": "samples/image001.jpg",
        "question": "изображение странное или нормальное?",
        "option_a": "нормальное",
        "option_b": "странное"
    },
    "outputs": "B",
    "meta": {
        "id": 0,
        "categories": {
            "commonsense_violation_subgroup": "Unusual dish",
            "commonsense_violation_group": "Food and Nutrition Mismatches"
        },
        "pair_id": "v5_7",
        "image": {
            "synt_source": [
                "gpt-4o",
                "dall-e-3"
            ],
            "source": "ai-generated"
        }
    }
}
```


### Создание датасета

Датасет был собран на основе оригинального бенчмарка [WHOOPS!](https://whoops-benchmark.github.io/) при помощи итеративной синтетической генерации в стиле [Self-Instruct](https://github.com/yizhongw/self-instruct). Каждый пример в сабсете WHOOPS! для бинарной классификации – это "странное" и "нормальное" изображение, а также категория противоречия здравому смыслу и описание каждого изображения. Для расширения датасета мы итеративно генерировали новые категории и описания изображений через GPT-4o, используя примеры из WHOOPS! в качестве few-shots. Далее, по сгенерированным описаниям мы генерировали изображения через DALL-E, вручную отфильтровывали неудачные генерации, и добавляли удачные в пул, чтобы использовать их затем как примеры для новых генераций.


## Оценка

### Метрики

Для агрегированной оценки ответов моделей используются следующие метрики:

- `Exact match`: Метрика Exact match вычисляет среднее по оценкам всех обработанных вопросов, где оценка имеет значение 1, если предсказанная строка точно совпадает с правильным ответом, и 0 в остальных случаях.


### Human baseline

Human baseline — это оценка усредненных ответов людей на вопросы бенчмарка. Оценка проводится по тем же метрикам, что и для моделей.

Ручная разметка проходила на платформе Яндекс Задания с перекрытием в 5 аннотаторов. Было размечено 80 контрольных заданий и 10 обучающих. Аннотаторы, не прошедшие как минимум 80% обучения правильно, не допускались до разметки. Аннотаторы, допустившие ошибки хотя бы в 5 контрольных заданиях, отстранялись от разметки. Агрегированным ответом считался тот, который был выбран большинством (majority vote).

Результаты оценки:

- Exact match – 0.85
