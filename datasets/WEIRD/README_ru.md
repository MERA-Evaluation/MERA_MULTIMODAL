# WEIRD


## Описание задачи

WEIRD – это расширенная версия подзадачи бинарной классификации оригинального английского бенчмарка [WHOOPS!](https://whoops-benchmark.github.io/). Датасет оценивает, способна ли мультимодальная модель обнаруживать нарушения здравого смысла в изображениях. Здесь нарушение здравого смысла – это ситуации, противоречащие типичным нормам реальности. Например, пингвины не могут летать, дети не водят автомобили, посетители не накладывают еду официантам, и так далее. В датасете поровну "странных" и обычных изображений.

Тестируемые навыки моделей: Weirdness understanding, Common everyday knowledge, Physical property understanding, Object function understanding, Identity & emotion understanding, Other inductive reasoning

Авторы: Елисей Рыков, Василий Коновалов, Александр Панченко


## Мотивация

Датасет ориентирован на проверку навыков оценки реальности изображения и подходит для оценки любых AI-моделей, способных анализировать изображение. Ключевая оцениваемая способность – это анализ визуальной информации изображения и сопоставление ее с привычными нормами реальности. Метрика оценки качества – accuracy. Так как датасет оценивает фундментальные способности модели оценивать "реальность", он будет интересен всем исследователям как один из базовых этапов проверки модели.


## Описание датасета

### Поля данных

Каждый вопрос в датасете содержит следующие поля:

- `instruction` [str] — Промпт-инструкция для модели, содержащая шаблон для вставки элементов вопроса.
- `inputs` — Вводные данные, формирующие задание для модели. Могут включать одну или несколько модальностей - видео, аудио, изображение, текст.
    - `image` [str] — Путь к файлу с изображением, к которому относится вопрос.
    - `question` [str] — Текст вопроса.
    - `option_a` [str] — Вариант ответа A.
    - `option_b` [str] — Вариант ответа B.
- `outputs` [str] — Правильный ответ на вопрос.
- `meta` — Метаданные, относящиеся к тестовому примеру, но не используемые в вопросе (скрытые от тестируемой модели).
    - `id` [int] — Номер-идентификатор вопроса в датасете.
    - `categories` — Категории признаков, характеризующих тестовый пример.
        - `commonsense_violation_subgroup` [str] — Подгруппа нарушения норм реальности, сгенерированная синтетически;
        - `commonsense_violation_group` [str] — Общая группа нарушения норм реальности, полученная вручную путем объединением подгрупп;
    - `caption` [str] — Текстовое описание, используемое для генерации изображения;
    - `pair_id` [str] — Номер пары изображений;
    - `image` — Метаданные, относящиеся к изображению.
        - `synt_source` [list] — Модели, используемые для генерации вопроса;
        - `source` [str] — Источник изображения


### Пример данных

```json
{
    "instruction": "Внимательно изучите предложенное изображение.\n<image>.\n Вопрос: {question}. Странные изображения противоречат здравому смыслу, в то время как нормальные ему соответствуют. В качестве ответа укажите только букву A или B без дополнительных рассуждений.\n\nA. {option_a}\nB. {option_b}\nОтвет:",
    "inputs": {
        "image": "samples/image0.jpg",
        "question": "изображение странное или нормальное?",
        "option_a": "странное",
        "option_b": "нормальное"
    },
    "outputs": "A",
    "meta": {
        "id": 0,
        "categories": {
            "commonsense_violation_subgroup": "Object Misplacement",
            "commonsense_violation_group": "Object Function and Misuse"
        },
        "caption": "A plate of spaghetti served on a dinner table with a fork.",
        "pair_id": "v10_47",
        "image": {
            "synt_source": [
                "gpt-4o",
                "dall-e-3"
            ],
            "source": "ai-generated"
        }
    }
}
```


### Промпты

Для задачи были подготовлены 10 промптов, которые были равномерно распределены по вопросам по принципу "один вопрос – один промпт". Шаблоны в фигурных скобках в промпте заполняются из полей внутри поля `inputs` в каждом вопросе.


Пример промпта:

```
Внимательно изучите предложенное изображение.
<image>.
 Вопрос: {question}. Странные изображения противоречат здравому смыслу, в то время как нормальные ему соответствуют. В качестве ответа укажите только букву A или B без дополнительных рассуждений.

A. {option_a}
B. {option_b}
Ответ:
```


### Создание датасета

Датасет был собран на основе оригинального бенчмарка [WHOOPS!](https://whoops-benchmark.github.io/) при помощи итеративной синтетической генерации в стиле [Self-Instruct](https://github.com/yizhongw/self-instruct). Каждый пример в сабсете WHOOPS! для бинарной классификации – это "странное" и "нормальное" изображение, а также категория противоречия здравому смыслу и описание каждого изображения. Для расширения датасета мы итеративно генерировали новые категории и описания изображений через GPT-4o, используя примеры из WHOOPS! в качестве few-shots. Далее, по сгенерированным описаниям мы генерировали изображения через DALL-E, вручную отфильтровывали неудачные генерации, и добавляли удачные в пул, чтобы использовать их затем как примеры для новых генераций.


## Оценка


### Метрики

Для агрегированной оценки ответов моделей используются следующие метрики:

- `Accuracy`: Метрика Accuracy вычисляет долю правильных предсказаний модели среди всех обработанных вопросов.


### Human baseline

Для сравнения качества ответов моделей с тем, как отвечают люди, были проведены замеры ответов людей по следующей методологии.

Ручная разметка проходила на платформе Яндекс Задания с перекрытием в 5 аннотаторов. Было размечено 80 контрольных заданий и 10 обучающих. Аннотаторы, не прошедшие как минимум 80% обучения правильно, не допускались до разметки. Аннотаторы, допустившие ошибки хотя бы в 5 контрольных заданиях, отстранялись от разметки. В результате коэффициент согласованности альфа Криппендорфа равен 0.69, а точность человека – 82.22%

Результаты оценки:

- Accuracy – 82.22
