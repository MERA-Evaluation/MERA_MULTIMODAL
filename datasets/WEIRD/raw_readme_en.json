{
    "Task description": "WEIRD is an extended version of a binary classification subtask of the original English [WHOOPS!](https://whoops-benchmark.github.io/) benchmark. The dataset evaluates the ability to detect violations of commonsense. Commonsense violations are situations that contradict the norm of reality. For example, penguins can't fly, children don't drive cars, guests don't set the table for the waiter, etc. \"Weird\" and \"normal\" images are equally distributed in the dataset.",
    "Motivation": "The dataset focuses on the evaluation of the reality check, and is suitable for the evaluation of any AI models that can analyze images. The main capability that this dataset evaluates is the analysis of the visual information and collating it with the common sense. Accuracy is the main evaluation metric. Since the dataset evaluates the basic ability of reality check, it will be interesting for any research as one of the basic stages of the model evaluation pipeline.",
    "Dataset creation": "The dataset was created based on the original [WHOOPS!](https://whoops-benchmark.github.io/) benchmark, using iterative synthetic generation in the style of [Self-Instruct](https://github.com/yizhongw/self-instruct). Each sample from the WHOOPS! subset for binary classification is a pair consisting of a \"weird\" and a \"normal\" image, along with categories of commonsense violations and image descriptions. To extend the original benchmark, we iteratively generated new categories of commonsense violation and image descriptions using GPT-4o with WHOOPS! samples as a few shots. In addition, we used synthetic descriptions to generate images using DALL-E. Next, we manually filtered out bad images and added good images to the pool. Finally, the pool was used to repeat the generation process and extract new few-shots.",
    "Human baseline": "The human baseline was annotated on the Yandex Tasks platform with an overlap of 5 annotators. 80 control tasks and 10 training tasks were added. Annotators who did not complete at least 80% of the training tasks correctly were not allowed to annotate. Annotators with errors in 5 or more control tasks were excluded from markup. This resulted in a Krippendorff's alpha coefficient of consistency of 0.69 and a human accuracy of 82.22%.",
    "Contributors": "Elisei Rykov, Vasily Konovalov, Alexander Panchenko"
}