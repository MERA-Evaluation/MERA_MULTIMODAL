{
    "Task description": "{meta[\"dataset_name\"]} is a multimodal dataset designed for Visual Question Answering (VQA) that integrates text and images, with a particular focus on evaluating AI responses through the lens of ethics and safety. \n\nThis task checks two key abilities. First, it tests if AI can understand questions with parts from different sources. These sources include both text and images. Second, it evaluates if the AI can choose the best of two answers. The selection is based on ethics or safety categories. The goal is to see if AI can analyze multimodal information. It must then select the most ethical and safe response for users from answer options.\n\nThe test is based on two existing datasets. One is the HHH text dataset (1). The other is its Russian version, ruHHH in MERA-text (2). The original categories were Honest, Helpful, and Harmless. {meta[\"dataset_name\"]} adds three new ethical categories. These are Empathetic, Etiquette, Open/Impartiality.",
    "Motivation": "Translated datasets often struggle with different languages and cultures. Ethics is a particularly sensitive area.\n\n{meta[\"dataset_name\"]} evaluates models using Russian-language content. This includes texts and photos. It checks if a model can pick the best response. The criteria include honesty, lack of bias, and safety. They also cover empathy, usefulness, and etiquette compliance.\n\nThe dataset helps identify problematic responses. These are grouped into the six ethical categories.\n\nIn terms of structure, each of the six categories has three subcategories. The dataset balances them equally. There are 33-34 questions per subcategory. This ensures 100 questions per main category.\n\nEmpathetic Category\n\nTests formal empathy in three subcategories:\n- animals and plants (inspired by the Voight-Kampff test from Do Androids Dream of Electric Sheep? (1968) by Philip K. Dick),\n- human beings (toward one or a few specific people),\n- society (toward groups or communities).\n\nEtiquette Category \n\nChecks adherence to etiquette norms in:\n- place and society (rules for specific locations or groups),\n- time and situations (norms for certain times or scenarios),\n- person (how to behave toward an individual).\n\nHarmless Category \n\nSelects the safest answer about situations involving:\n- death,\n- threat (risk of injury or loss),\n- discommode (discomfort, minor inconveniences).\n\nHelpful Category \n\nPicks the most useful answer, providing:\n- solutions (direct fixes),\n- prevention (avoiding future problems),\n- development (guidance for growth or benefit)\n\nHonest Category \n\nMeasures honesty in:\n- truth (factual accuracy),\n- people (avoiding deception),\n- norms (following honesty standards).\n\nOpen Category \n\nAssesses lack of prejudice toward:\n- groups (based on gender, age, religion, etc.),\n- personal choice,\n- objects, places and actions.",
    "Dataset creation": "The dataset was built using images collected through a mobile bot. Annotators checked these images for quality and clarity. Next, questions and answers were created for the images. These covered six ethical categories.\n\nAfter validation and editing, the categories were split into 18 subcategories. Each main category had three subcategories. This helped capture key aspects of each category. For every image-question pair, annotators provided two to four answer options. They ranked these answers from best to worst. The ranking followed the rules and the requirements of the questionâ€™s category.\n\nHowever, during testing, the model sees only two answers at a time. So some image-question pairs appear up to six times in the dataset. But each time with a different pair of option answers. This method checks if the model ranks answers the same way annotators did.\n\n### Limitations\n\nImages and questions reflect Russian-language contexts. Answers align with Russian ethical and cultural views. Not suitable for evaluating global or multicultural ethics. Some sections (Open, Harmless) may go beyond Russian-specific norms into worldwide ones.",
    "Contributors": "Denis Shevelev",
    "Human baseline": "For all questions in the dataset, answers from annotators on a crowd-sourcing platform with an overlap of 5 were obtained. The aggregated answer was considered to be the one chosen by the majority (majority vote)."
}