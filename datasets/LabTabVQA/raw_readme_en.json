{
  "Task description": "LabTabVQA is a Russian-language question-answering dataset based on images of tables from the medical domain. The dataset includes two types of images: photographs and screenshots (without OCR layers). Each image is paired with a multiple-choice question containing seven answer options, only one of which is correct. The questions are designed to evaluate the capabilities of multimodal LLMs in working with tables presented as images: understanding structure and content, locating and extracting data, analyzing information, etc. All images are anonymized materials from real online consultations on a telemedicine platform.",
  "Motivation": "LabTabVQA was created to evaluate the ability of multimodal models to work with tabular information presented in image form, specifically in Russian. Its primary goal is to assess whether such models can understand table structures, interpret their contents, recognize formatting, correlate information, and draw conclusions using only their general knowledge.\n\nThe dataset creation and question-generation methodology is not limited to a specific domain and can be extended to include tables from related areas of knowledge. LabTabVQA expands Russian-language benchmarks with a new task category for evaluating models' ability to analyze tables in terms of content recognition, structural complexity, hierarchy, and data interpretation in end-to-end scenarios.",
  "Dataset creation": "The dataset was built using 697 real images from a telemedicine consultation platform.\n\nUsing the GPT-4o Mini model, we annotated images according to two binary criteria:\n\n- presence of a table in the image;\n\n- photo or screenshot.\n\n339 images were selected, balanced by image type and table size (also assessed using GPT-4o Mini). For 138 samples, questions were written by experts; for the remaining 201, questions were generated using an AI-agent system composed of the following components:\n\n1. QuestionGenerator (GPT-o4 Mini): generates a candidate question with 7 answer options based on the image and question category;\n\n2. QuestionQualifier (GPT-o4 Mini): identifies the correct answer among the 7 options, or requests regeneration if no correct option is found;\n\n3. Solvers (GPT-4o Mini): at three levels of difficulty (defined by prompts), answer the question and provide reasoning;\n\n4. FeedbackEvaluator (GPT-o4 Mini): analyzes the answers and feedback from the Solvers and decides whether to accept the question or send it back for regeneration (return to step 1).\n\nThe generated examples were validated on the TagMe platform (with 3-way overlap) based on the following criteria:\n\n- the question is based on the table shown in the image;\n\n- the question does not require domain-specific knowledge (all required information is in the image/table);\n\n- the question cannot be answered without using the table/image.\n\nSimilarly, the correct answer was selected by assessors. A correct answer was defined as:\n\n- the answer proposed by the question generation system, if at least 2 out of 3 assessors agreed with it;\n\n- the answer chosen by at least 2 out of 3 assessors, even if it differed from the generated answer, provided it was additionally validated by a meta-assessor.\n\nDue to the specifics of the question-generation methodology, the dataset and tasks may be biased toward the GPT-o4 model family.",
  "Human baseline": "The human baseline was established via independent annotation on the TagMe platform, with 5-way overlap. Assessors were asked to answer the generated questions, and the instructions included annotated examples (the same used for few-shot prompting). The final prediction was determined by majority vote among assessors' responses (at least 3 out of 5 must match), and the metric amounted to 93.9%. In 11 samples (3.35% of all data), a majority vote could not be reached, so they were excluded from evaluation.",
  "Contributors": "Amina Miftakhova, Ivan Sviridov"
}