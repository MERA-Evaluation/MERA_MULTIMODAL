# XXX


## Описание задачи

Описание задачи.

Тестируемые навыки моделей: Counterfactual robustness, Static counting

Авторы: Имя Фамилия, Имя Фамилия


## Мотивация

Этот раздел должен разносторонне отвечать на вопрос “зачем эта задача?”. Вопросы для ориентира (см. фреймворк [ECBD](https://aclanthology.org/2024.acl-long.861/)):

- На оценку каких моделей ориентирован этот датасет? Для каких моделей он НЕ подходит (limitations)?
- На каких пользователей ориентированы результаты оценки на данной задаче? Как эти пользователи будут интерпретировать результаты, полученные при оценке на этом датасете?
- Какие способности моделей оценивает задача? Что подразумевается под этими способностями? Не просто language understanding, а что именно и в какой постановке. Зачем оценивать именно эти способности?
- Как устроены вопросы в датасете, и почему они устроены именно так? Как понять, что именно такой дизайн задачи оценивает те способности моделей, которые нужно оценить? Валидность эксперимента.
- Как выбраны метрики (особенно когда метрика не просто доля правильных ответов) и почему они именно такие? Как такой выбор метрик помогает пользователю интерпретировать результаты?


## Описание датасета

### Поля данных

Каждый вопрос в датасете содержит следующие поля:

- `instruction` [str] — Промпт-инструкция для модели, содержащая шаблон для вставки элементов вопроса.
- `inputs` — Вводные данные, формирующие задание для модели. Могут включать одну или несколько модальностей - видео, аудио, изображение, текст.
    - `question` [str] — Текст вопроса.
    - `image` [str] — Путь к файлу с изображением, к которому относится вопрос.
    - `option_a` [str] — Вариант ответа A.
    - `option_b` [str] — Вариант ответа B.
    - `option_c` [str] — Вариант ответа C.
    - `option_d` [str] — Вариант ответа D.
- `outputs` [str] — Правильный ответ на вопрос.
- `meta` — Метаданные, относящиеся к тестовому примеру, но не используемые в вопросе (скрытые от тестируемой модели).
    - `id` [int] — Номер-идентификатор вопроса в датасете.
    - `categories` — Категории признаков, характеризующих тестовый пример.
        - `task_type` [str] — Тип задач в соответствии с классификацией задач в датасете XXX;
    - `image` — Метаданные, относящиеся к изображению.
        - `synt_source` [list] — Источники, с помощью которых сгенерированы или воссозданы данные для формирования вопроса, в том числе названия генеративных моделей.
        - `source` [list] — Информация о происхождении изображения — согласно классификации изображений для датасетов MERA.
        - `type` [list] — Тип изображения — согласно классификации изображений для датасетов MERA.
        - `content` [list] — Содержание изображения — согласно классификации изображений для датасетов MERA.
        - `context` [list] — Сопроводительный контекст, присутствующий на изображении, — согласно классификации изображений для датасетов MERA.


### Пример данных

```json
{
    "instruction": "Посмотри на картинку <image> и ответь на вопрос, выбрав вариант ответа из предложенных. Напиши только букву правильного ответа.\nВопрос: {question}.\nA. {option_a}\nB. {option_b}\nC. {option_c}\nD. {option_d)\nОтвет:",
    "inputs": {
        "question": "Сколько автомобилей изображено на фото?",
        "image": "image0001.jpg",
        "option_a": "три",
        "option_b": "два",
        "option_c": "ни одного",
        "option_d": "пять"
    },
    "outputs": "C",
    "meta": {
        "id": 1,
        "categories": {
            "task_type": "counterfactual"
        },
        "image": {
            "synt_source": [
                "model-name"
            ],
            "source": [
                "photo"
            ],
            "type": [
                "visual"
            ],
            "content": [
                "view",
                "objects"
            ],
            "context": [
                "no_context"
            ]
        }
    }
}
```


### Промпты

Для задачи были подготовлены 10 промптов, которые были равномерно распределены по вопросам по принципу "один вопрос – один промпт". Шаблоны в фигурных скобках в промпте заполняются из полей внутри поля `inputs` в каждом вопросе.


Пример промпта:

```
prompt_0
```


### Создание датасета

Методология создания датасета.


## Оценка


### Метрики

Для агрегированной оценки ответов моделей используются следующие метрики:

- `Accuracy`: Метрика Accuracy вычисляет долю правильных предсказаний модели среди всех обработанных вопросов.


### Human baseline

Для сравнения качества ответов моделей с тем, как отвечают люди, были проведены замеры ответов людей по следующей методологии.

Методология проведения Human baseline.

Результаты оценки:

- Accuracy – 1.00
