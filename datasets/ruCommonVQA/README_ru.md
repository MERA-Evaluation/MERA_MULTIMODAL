# ruCommonVQA


## Описание задачи

ruCommonVQA — вопросно-ответный публичный датасет на русском языке по изображениям двух типов: фото и картинки. 
Вопросы делятся 1) на простые и 2) сложные, разбитые на самые частотные типы: бинарный, сравнительный, сколько, где, как, какой, что, кто, микс. Для простых вопросов нужно лишь ориентироваться на изображение, для сложных — сделать шаг ризонинга. Все изображения для сета — классические, из публичных источников, реальные фото и мультяшные абстрактные изображения. Датасет публичный, базовый VQA для русского языка.

Тестируемые навыки моделей: Scene understanding, Physical property understanding, Object function understanding, Identity & emotion understanding, Static counting, Common everyday knowledge, Spatial object relationship, Object-object interaction, Human-object interaction, Human-human interaction, Object localization, Object recognition, Living things motion recognition, Object motion recognition

Авторы: Мария Тихонова, Ульяна Исаева, Алена Феногенова


## Мотивация

Датасет решает классическую базовую задачу VQA, по аналогии с английскими датасетами [VQA](https://visualqa.org/download.html). Для русского языка нет в свободном доступе базового VQA сета, как бейзлайна для оценки картиночно-текстовых моделей. Данный датасет рассчитан на проверку базовых способностей моделей различать объекты на изображениях разного типа, понимание вопросов разного типа и генерировать ответ на основе картинки. Вопросы покрывают основные способности: понимание объектов на изображении Fine-grained Perception (Single instance), общее восприятие изображения Coarse perception, здравый смысл, общие знания. Так как картинки взяты из публичных источников ([COCO](https://cocodataset.org/) датасет, [VQA v2](https://huggingface.co/datasets/pingzhili/vqa_v2) английский), важно учитывать это при интерпретации оценки. Возможна косвенная утечка данных через картинки в данных обучениях моделей.


## Описание датасета

### Поля данных

Каждый вопрос в датасете содержит следующие поля:

- `instruction` [str] — Промпт-инструкция для модели, содержащая шаблон для вставки элементов вопроса.
- `inputs` — Вводные данные, формирующие задание для модели.
    - `image` [str] — Путь к файлу с изображением, к которому относится вопрос.
    - `question` [str] — Текст вопроса.
- `outputs` [str] — Правильный ответ на вопрос.
- `meta` — Метаданные, относящиеся к тестовому примеру, но не используемые в вопросе (скрытые от тестируемой модели).
    - `id` [int] — Номер-идентификатор вопроса в датасете.
    - `categories` — Категории признаков, характеризующих тестовый пример.
        - `question_type` [str] — типы вопросов: бинарный, сравнительный, сколько, где, как, какой, что, кто;
    - `image` — Метаданные, относящиеся к изображению.
        - `source` [list] — источник изображений: фото из COCO или абстрактная картинка;
    - `complexity` [str] — Сложность вопроса: простая или сложная


### Пример данных

```json
{
    "instruction": "Внимательно посмотрите на картинку <image>.\nОтветьте кратко на вопрос. В качестве ответа напишите слово в той же форме, как спрашивается в вопросе, без дополнительных рассуждений, либо цифру, если ответом является число.\nВопрос:{question}\nОтвет:",
    "inputs": {
        "image": "samples/image0001.jpg",
        "question": "На фото есть люди?"
    },
    "outputs": "Да",
    "meta": {
        "id": 123,
        "categories": {
            "question_type": "binary"
        },
        "image": {
            "source": [
                "photo"
            ]
        },
        "complexity": "simple"
    }
}
```


### Создание датасета

Датасет содержит данные из двух источников: одна часть включает примеры из оригинального англоязычного датасета VQA, вторая часть была собрана с нуля.

Первая часть содержит изображения из датасета [VQA v2](https://huggingface.co/datasets/pingzhili/vqa_v2), который включает изображения из [COCO](https://cocodataset.org). Для этой части аннотаторы вручную создали вопросы и ответы с помощью платформы ABC Elementary. Для каждого изображения было сформулировано три вопроса, при этом каждое изображение разметили три разных аннотатора. Полученные данные были агрегированы и автоматически отфильтрованы (удалены длинные ответы, опечатки и проблемы с форматированием), а также прошли ручную проверку.

Вторая часть датасета была собрана с нуля. Для сбора изображений использовался Telegram-бот с обязательным пользовательским соглашением, гарантирующим конфиденциальность фото и наличие добровольного согласия. Изображения собирались при условии, что загружаемые фото являются уникальными и ранее не публиковались в интернете или других открытых источниках.
Генерация вопросов и ответов для новых изображений, полученных через Telegram-бота, также осуществлялась через платформу ABC Elementary. Аннотаторам предоставлялись изображения, к которым нужно было придумать вопрос и соответствующий ответ.


## Оценка

### Метрики

Для агрегированной оценки ответов моделей используются следующие метрики:

- `Exact match`: Метрика Exact match вычисляет среднее по оценкам всех обработанных вопросов, где оценка имеет значение 1, если предсказанная строка точно совпадает с правильным ответом, и 0 в остальных случаях.


### Human baseline

Human baseline — это оценка усредненных ответов людей на вопросы бенчмарка. Оценка проводится по тем же метрикам, что и для моделей.

Для всех вопросов датасета были получены ответы разметчиков на crowd-source платформе с перекрытием 5. Ответы в свободной форме были нормализованы (регистр, пробелы) для сравнения с эталоном. Агрегированным ответом считался тот, который был выбран большинством (majority vote).

Результаты оценки:

- Exact match – 0.82
