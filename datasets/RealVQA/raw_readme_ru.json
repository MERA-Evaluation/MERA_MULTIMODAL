{
    "Описание задачи": "{meta[\"dataset_name\"]} — бенчмарк для проверки способности модели отвечать на вопросы по изображениям. Вопросы задаются на русском языке и могут как относиться к какому-то конкретному объекту на изображении, так и ко всему изображению в целом. Бенчмарк построен таким образом, что ответить на вопрос без изображения невозможно. Часто для получения ответа необходимо провести логическое рассуждение в несколько этапов. Ключевой особенностью датасета является наличие вопросов-ловушек. Такие вопросы либо посвящены объектам, которые не присутствуют на изображении, либо информации для ответа на вопрос заведомо недостаточно. Ожидаемым поведением модели в случае вопросов-ловушек является сообщение о том, что вопрос не может быть отвечен, а также указание на причину, по которой это нельзя сделать. Таким образом проверяется устойчивость модели к галлюцинациям.",
    "Мотивация": "Датасет разработан для оценки способности модели выявлять причинно-следственные связи и применять логическое мышление на основе изображений. Вопросы сформулированы так, что на них нельзя ответить без изображения. В отличие от классических VQA-датасетов, которые в основном оценивают способность модели к прямому восприятию объектов (то есть понимание простых форм и цветов), данный датасет включает наиболее сложные типы восприятия из таксономии MERA, в частности, понимание взаимосвязей между объектами и различные типы рассуждений. Главное требование – применение логики или рассуждений для нахождения правильного ответа.\nДатасет предназначен для современных моделей, работающих с изображениями и текстом, которые могут не только понимать изображённое, но и делать логические выводы. Это типичное требование для современных диалоговых систем, так как пользователи задают сложные вопросы об изображениях, на которые существуют однозначные ответы. Так как для ответа не требуются экспертные знания, датасет ориентирован на повседневные вопросы и изображения, которые можно легко отправить в чат.",
    "Создание датасета": "Для сбора изображений для датасета использовались Telegram-бот и пользовательское соглашение, гарантирующее конфиденциальность фотографий и получение согласия пользователей. Изображения собирались посредством краудсорсинга, при условии, что загружаемое изображение должно быть уникальным и ранее недоступным в интернете или публичных источниках.\nПервая часть проекта была направлена на генерацию вопросов и ответов с использованием платформы ABC Elementary. Вопросы составлялись тренерами ИИ. Аннотаторам предоставлялось изображение, и им предлагалось сформулировать вопрос и подобрать корректный ответ. Основное внимание уделялось сложным вопросам, которые определялись как соответствующие одному из следующих критериев: необходимость прослеживания причинно-следственных связей, понимание или восприятие взаимосвязей между объектами, либо требование дополнительного рассуждения для нахождения ответа. Знания, необходимые для ответа, ограничивались тем, что обычно входит в школьную программу и соответствует общим логическим принципам, то есть не требует использования специализированных экспертных знаний.\nКроме того, был создан отдельный проект на платформе ABC Elementary для вопросов-ловушек. Те же аннотаторы получали фотографии от Telegram-бота и формулировали вопросы, аналогичные тем, что были в первой части проекта, но касающиеся объектов, отсутствующих на изображениях.\nНа третьем этапе аннотации проводилась проверка сгенерированных вопросов и ответов. При этом с помощью платформы ABC Elementary использовался краудсорсинговый подход, при котором каждый вопрос проверялся тремя аннотаторами. Проверялись следующие аспекты: 1) на вопрос нельзя ответить без изображения; 2) вопрос не является слишком общим, бинарным или не требует экспертных знаний; 3) ответ является однозначным; 4) ответ соответствует требуемому формату; и 5) выбран правильный тип вопроса.\nВсе проекты были затем объединены, и итоговые данные были стандартизированы в единый формат. На этапе проверки в метаданные был дополнительно добавлен тип вопроса с категориями: `object_properties;logics,other;text_understanding;objects_relationship;knowledge`. Вопросы-ловушки составили 10% датасета.",
    "Авторы": "Ульяна Исаева, Александр Харитонов, Ярослав Гребняк, Алена Феногенова",
    "Human baseline": "Для всех вопросов датасета были получены ответы разметчиков на crowd-source платформе с перекрытием 5. Ответы в свободной форме были нормализованы (регистр, пробелы) для сравнения с эталоном. Агрегированным ответом считался тот, который был выбран большинством (majority vote)."
}