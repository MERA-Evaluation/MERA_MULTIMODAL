# RealVQA


## Описание задачи

RealVQA — бенчмарк для проверки способности модели отвечать на вопросы по изображениям. Вопросы задаются на русском языке и могут как относиться к какому-то конкретному объекту на изображении, так и ко всему изображению в целом. Бенчмарк построен таким образом, что ответить на вопрос без изображения невозможно. Часто для получения ответа необходимо провести логическое рассуждение в несколько этапов. Ключевой особенностью датасета является наличие вопросов-ловушек. Такие вопросы либо посвящены объектам, которые не присутствуют на изображении, либо информации для ответа на вопрос заведомо недостаточно. Ожидаемым поведением модели в случае вопросов-ловушек является сообщение о том, что вопрос не может быть отвечен, а также указание на причину, по которой это нельзя сделать. Таким образом проверяется устойчивость модели к галлюцинациям.

Тестируемые навыки моделей: Spatial object relationship, Object-object interaction, Human-object interaction, Object localization, Object recognition, Object motion recognition, Living things motion recognition, Common everyday knowledge, Common domain knowledge, Style & genre understanding, Scene understanding, Physical property understanding, Object function understanding, Hypothetical reasoning, Cause & effect understanding, Static counting, Mathematical reasoning, Counterfactual robustness, Problem decomposition, Comparative reasoning

Авторы: Ульяна Исаева, Александр Харитонов, Ярослав Гребняк, Алена Феногенова


## Мотивация

Датасет разработан для оценки способности модели выявлять причинно-следственные связи и применять логическое мышление на основе изображений. Вопросы сформулированы так, что на них нельзя ответить без изображения. В отличие от классических VQA-датасетов, которые в основном оценивают способность модели к прямому восприятию объектов (то есть понимание простых форм и цветов), данный датасет включает наиболее сложные типы восприятия из таксономии MERA, в частности, понимание взаимосвязей между объектами и различные типы рассуждений. Главное требование – применение логики или рассуждений для нахождения правильного ответа.
Датасет предназначен для современных моделей, работающих с изображениями и текстом, которые могут не только понимать изображённое, но и делать логические выводы. Это типичное требование для современных диалоговых систем, так как пользователи задают сложные вопросы об изображениях, на которые существуют однозначные ответы. Так как для ответа не требуются экспертные знания, датасет ориентирован на повседневные вопросы и изображения, которые можно легко отправить в чат.


## Описание датасета

### Поля данных

Каждый вопрос в датасете содержит следующие поля:

- `instruction` [str] — Промпт-инструкция для модели, содержащая шаблон для вставки элементов вопроса.
- `inputs` — Вводные данные, формирующие задание для модели.
    - `image` [str] — Путь к файлу с изображением, к которому относится вопрос.
    - `question` [str] — Текст вопроса.
- `outputs` [str] — Правильный ответ на вопрос.
- `meta` — Метаданные, относящиеся к тестовому примеру, но не используемые в вопросе (скрытые от тестируемой модели).
    - `id` [int] — Номер-идентификатор вопроса в датасете.
    - `categories` — Категории признаков, характеризующих тестовый пример.
        - `question_topic` [list] — основные темы вопроса (например, text_understanding, logic и т.д.);
        - `domain` [list] — домены изображения (например, plant, music, sport, interior и т.д.);
    - `image` — Метаданные, относящиеся к изображению.
        - `source` [list] — Информация о происхождении изображения — согласно классификации изображений для датасетов MERA.
        - `type` [list] — Тип изображения — согласно классификации изображений для датасетов MERA.
        - `content` [list] — Содержание изображения — согласно классификации изображений для датасетов MERA.
        - `context` [list] — Сопроводительный контекст, присутствующий на изображении, — согласно классификации изображений для датасетов MERA.


### Пример данных

```json
{
    "instruction": "Внимательно посмотрите на картинку <image>.\nОтветьте на заданный вопрос кратко. В качестве ответа на вопрос напишите слово в той же форме, как спрашивается в вопросе, без дополнительных рассуждений.\n\nВопрос:{question}\n\nОтвет:",
    "inputs": {
        "image": "samples/image0165.jpg",
        "question": "Предположительно в какой день недели сделано это фото?"
    },
    "outputs": "в пятницу",
    "meta": {
        "id": 165,
        "categories": {
            "question_topic": [
                "text_understanding"
            ],
            "domain": [
                "tecnology"
            ]
        },
        "image": {
            "source": [
                "photo"
            ],
            "type": [
                "photo",
                "text",
                "inventory_code"
            ],
            "content": [
                "object"
            ],
            "context": [
                "no_context"
            ]
        }
    }
}
```


### Создание датасета

Для сбора изображений для датасета использовались Telegram-бот и пользовательское соглашение, гарантирующее конфиденциальность фотографий и получение согласия пользователей. Изображения собирались посредством краудсорсинга, при условии, что загружаемое изображение должно быть уникальным и ранее недоступным в интернете или публичных источниках.
Первая часть проекта была направлена на генерацию вопросов и ответов с использованием платформы ABC Elementary. Вопросы составлялись тренерами ИИ. Аннотаторам предоставлялось изображение, и им предлагалось сформулировать вопрос и подобрать корректный ответ. Основное внимание уделялось сложным вопросам, которые определялись как соответствующие одному из следующих критериев: необходимость прослеживания причинно-следственных связей, понимание или восприятие взаимосвязей между объектами, либо требование дополнительного рассуждения для нахождения ответа. Знания, необходимые для ответа, ограничивались тем, что обычно входит в школьную программу и соответствует общим логическим принципам, то есть не требует использования специализированных экспертных знаний.
Кроме того, был создан отдельный проект на платформе ABC Elementary для вопросов-ловушек. Те же аннотаторы получали фотографии от Telegram-бота и формулировали вопросы, аналогичные тем, что были в первой части проекта, но касающиеся объектов, отсутствующих на изображениях.
На третьем этапе аннотации проводилась проверка сгенерированных вопросов и ответов. При этом с помощью платформы ABC Elementary использовался краудсорсинговый подход, при котором каждый вопрос проверялся тремя аннотаторами. Проверялись следующие аспекты: 1) на вопрос нельзя ответить без изображения; 2) вопрос не является слишком общим, бинарным или не требует экспертных знаний; 3) ответ является однозначным; 4) ответ соответствует требуемому формату; и 5) выбран правильный тип вопроса.
Все проекты были затем объединены, и итоговые данные были стандартизированы в единый формат. На этапе проверки в метаданные был дополнительно добавлен тип вопроса с категориями: `object_properties;logics,other;text_understanding;objects_relationship;knowledge`. Вопросы-ловушки составили 10% датасета.


## Оценка

### Метрики

Для агрегированной оценки ответов моделей используются следующие метрики:

- `Exact match`: Метрика Exact match вычисляет среднее по оценкам всех обработанных вопросов, где оценка имеет значение 1, если предсказанная строка точно совпадает с правильным ответом, и 0 в остальных случаях.


### Human baseline

Human baseline — это оценка усредненных ответов людей на вопросы бенчмарка. Оценка проводится по тем же метрикам, что и для моделей.

Для всех вопросов датасета были получены ответы разметчиков на crowd-source платформе с перекрытием 5. Ответы в свободной форме были нормализованы (регистр, пробелы) для сравнения с эталоном. Агрегированным ответом считался тот, который был выбран большинством (majority vote).

Результаты оценки:

- Exact match – 0.58
