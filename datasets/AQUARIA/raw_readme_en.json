{
    "Task description": "The dataset includes multiple-choice questions that test complex audio comprehension, including speech, non-verbal sounds, and music. The tasks in the dataset require not only the recognition of speech but also the analysis of the entire auditory situation and the interactions among its components. The audio tracks used in AQUARIA were created specifically for this dataset.\n\nThe dataset contains 9 types of tasks:\n- Audio scene classification\n- Audio captioning (matching audio with its textual description)\n- Audio comparison (finding differences between two audio)\n- Audio sequence analysis\n- Emotion recognition (recognition of emotions and subjective characteristics of a speaker)\n- Sound QA (questions related to analysis of non-verbal signals)\n- Speaker characterization (recognition of objective characteristics of a speaker)\n- Music QA (questions requiring analysis of music and related knowledge)\n- Music characterization (recognition of objective characteristics of music)",
    "Motivation": "The methodology for evaluating large audio-language models (LALMs), as well as the models themselves, is a fairly recent area of research. Compared to benchmarks in the vision-language domain, there are significantly fewer comprehensive benchmarks available for evaluating audio-language models. Examples of such benchmarks include [AIR-Bench (02.2024)](https://arxiv.org/abs/2402.07729), [AudioBench (06.2024)](https://arxiv.org/abs/2406.16020), and [MMAU (10.2024)](https://arxiv.org/abs/2410.19168v1). Audio understanding tasks are generally classified into three categories: speech analysis, non-verbal signal analysis, and music analysis.\n\nThe AQUARIA dataset was developed to evaluate LALMs in Russian-language tasks. The model needs to be able to process audio because answering questions requires analyzing the associated audio track. The dataset contains 9 question types, which vary both by task category and by the model abilities they test. The dataset assesses three skill categories for audio-language models: perception, knowledge, and reasoning.",
    "Dataset creation": "Based on an analysis of existing benchmarks for testing language models with audio interfaces, we have developed 9 types of tasks that evaluate various groups of skills for these models. For each task type, experts created scenarios with dialogues, background sounds, and music, along with corresponding questions tailored to different task formulations. All scenarios were recorded using professional studio recording equipment, with voluntary use of dataset contributors' voices. For some of the Music QA and Music characterization questions, the music tracks were created using generative models (including suno.com).",
    "Contributors": "Ulyana Isaeva",
    "Human baseline": "For all questions in the dataset, answers from annotators on a crowd-sourcing platform with an overlap of 5 were obtained. The aggregated answer was considered to be the one chosen by the majority (majority vote)."
}