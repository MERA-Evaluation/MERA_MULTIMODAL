# AQUARIA


## Описание задачи

Датасет состоит из вопросов с выбором ответа, проверяющие комплексное понимание аудио, в том числе речи, неречевых сигналов и музыки. Вопросы датасета составлялись таким образом, чтобы для ответа на них требовалось не только распознавать речь, но и анализировать аудиоситуацию целиком и взаимодействие её компонентов. Используемые аудиофайлы созданы специально для датасета AQUARIA.

В датасете представлены вопросы 9 типов:
- Audio scene classification (классификация аудиосцены)
- Audio captioning (сопоставление аудио с текстовым описанием)
- Audio comparison (нахождение различий в паре аудио)
- Audio sequence analysis (анализ цепочки звуковых событий)
- Emotion recognition (определение эмоций и субъективных характеристик говорящего)
- Sound QA (вопросы на анализ неречевых сигналов)
- Speaker characterization (определение объективных характеристик говорящего)
- Music QA (вопросы на анализ и знания музыки)
- Music characterization (определение объективных характеристик музыки)

Тестируемые навыки моделей: Speaker diarization, Temporal object relationship, Object-object interaction, Human-object interaction, Human-human interaction, Object recognition, Object motion recognition, Living things motion recognition, Speech recognition, Common everyday knowledge, Common domain knowledge, Speech emotion recognition, Music emotion recognition, Style & genre understanding, Scene understanding, Physical property understanding, Identity & emotion understanding, Cause & effect understanding, Temporal counting, Comparative reasoning

Авторы: Ульяна Исаева


## Мотивация

Методология оценки больших аудио-языковых моделей (large audio language models, LALMs), как и сами такие модели, — относительно новая область исследований. По сравнению с vision-language доменом, существует меньше крупных бенчмарков, объединяющих разнообразные задачи для оценки навыков LALMs. Примерами таких бенчмарков являются [AIR-Bench (02.2024)](https://arxiv.org/abs/2402.07729), [AudioBench (06.2024)](https://arxiv.org/abs/2406.16020) и [MMAU (10.2024)](https://arxiv.org/abs/2410.19168v1). За основу классификации задач на понимание аудио можно принять разделение задач на анализ речи, неречевых сигналов и музыки.

Датасет разработан для оценки LALMs в задачах на русском языке. Для оценки на этом датасете модели необходим аудиоинтерфейс, так как для ответа на вопрос, заданный текстом, требуется анализ связанной с ним аудиодорожки. В датасете представлены вопросы 9 типов, отличающиеся постановкой задачи и тестируемыми способностями моделей. Датасет тестирует 3 группы навыков аудио-языковых моделей: восприятие звука (perception), знания (knowledge) и способность к рассуждению (reasoning).


## Описание датасета

### Поля данных

Каждый вопрос в датасете содержит следующие поля:

- `instruction` [str] — Промпт-инструкция для модели, содержащая шаблон для вставки элементов вопроса.
- `inputs` — Вводные данные, формирующие задание для модели.
    - `audio_1` [str] — Путь к файлу с аудио, к которому относится вопрос.
    - `audio_2` [str] — Путь ко второму файлу с аудио, к которому относится вопрос (в случае вопроса с двумя аудио файлами, иначе поле не используется).
    - `question` [str] — Текст вопроса.
    - `option_a` [str] — Вариант ответа A.
    - `option_b` [str] — Вариант ответа B.
    - `option_c` [str] — Вариант ответа C.
    - `option_d` [str] — Вариант ответа D.
- `outputs` [str] — Правильный ответ на вопрос.
- `meta` — Метаданные, относящиеся к тестовому примеру, но не используемые в вопросе (скрытые от тестируемой модели).
    - `id` [int] — Номер-идентификатор вопроса в датасете.
    - `categories` — Категории признаков, характеризующих тестовый пример.
        - `task_type` [str] — Тип задачи (см. раздел Описание задачи).


### Пример данных

```json
{
    "instruction": "Задание содержит две аудиозаписи и вопрос к ним с четырьмя вариантами ответа: A, B, C, D. Из них только один правильный. Прослушайте аудио: <audio_1>, <audio_2>. Прочитайте вопрос к аудиозаписям и напишите букву правильного ответа: {question}\nA) {option_a}\nB) {option_b}\nC) {option_c}\nD) {option_d}\n\nОтвет:",
    "inputs": {
        "question": "В чём различие двух предложенных аудиозаписей?",
        "audio_1": "samples/audio194.wav",
        "audio_2": "samples/audio195.wav",
        "option_a": "На первой записи отпирают дверь, на второй она была отперта",
        "option_b": "На первой записи дверь скрипит, на второй нет",
        "option_c": "На первой записи в квартиру входит женщина, на второй — мужчина",
        "option_d": "На первой записи человек заходит в открытую дверь, а на второй отпирает замок"
    },
    "outputs": "B",
    "meta": {
        "id": 173,
        "categories": {
            "task_type": "Audio comparison"
        }
    }
}
```


### Создание датасета

На основании анализа существующих бенчмарков для тестирования языковых моделей с аудиоинтерфейсом были разработаны 9 типов задач, которые тестируют разные группы навыков таких моделей. Для каждого типа эксперты составили сценарии ситуаций с диалогами, фоновыми звуками и музыкой. К сценариям были подобраны вопросы, соответствующие разным постановкам задачи. Для всех сценариев были записаны аудиодорожки в профессиональной студии звукозаписи, с добровольным использованием голосов авторов датасета. Для ряда вопросов по музыке использованы музыкальные треки, созданные с помощью генеративных моделей (suno.com и других).


## Оценка

### Метрики

Для агрегированной оценки ответов моделей используются следующие метрики:

- `Exact match`: Метрика Exact match вычисляет среднее по оценкам всех обработанных вопросов, где оценка имеет значение 1, если предсказанная строка точно совпадает с правильным ответом, и 0 в остальных случаях.


### Human baseline

Human baseline — это оценка усредненных ответов людей на вопросы бенчмарка. Оценка проводится по тем же метрикам, что и для моделей.

Для всех вопросов датасета были получены ответы разметчиков на crowd-source платформе с перекрытием 5. Агрегированным ответом считался тот, который был выбран большинством (majority vote).

Результаты оценки:

- Exact match – 0.98
