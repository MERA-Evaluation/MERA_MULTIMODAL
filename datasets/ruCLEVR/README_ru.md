# ruCLEVR


## Описание задачи

RuCLEVR — это датасет для задачи визуального вопросно-ответного ризонинга (Visual Question Answering, VQA), созданный по методологии [CLEVR](https://cs.stanford.edu/people/jcjohns/clevr/), адаптированной для русского языка.

RuCLEVR состоит из автоматически сгенерированных изображений 3D-объектов, каждый из которых характеризуется такими признаками, как форма, размер, цвет и материал, расположенных в различных условиях и образующих сложное визуальное окружение. Набор данных включает вопросы, основанные на этих изображениях и разбитые на определенные группы, такие как запрос атрибутов, сравнение атрибутов, существование, подсчет и целочисленное сравнение. Для создания вопросов использованы предопределённые шаблоны, что позволяет обеспечить последовательность и разнообразие. Датасет был создан с нуля, чтобы избежать предвзятости модели. Вопросы предназначены для оценки способности моделей выполнять задачи, требующие точного визуального рассуждения, анализируя признаки и отношения объектов в каждой сцене. Благодаря такому структурированному дизайну датасет обеспечивает контролируемую среду для оценки навыков точного рассуждения моделей при работе с визуальными данными.

Тестируемые навыки моделей: Common everyday knowledge, Spatial object relationship, Object recognition, Physical property understanding, Static counting, Comparative reasoning

Авторы: Ксения Бирюкова, Дарья Челонокова, Джамиля Эркенова, Артем Червяков, Мария Тихонова.


## Мотивация

Датасет RuCLEVR был создан для оценки возможностей визуальных рассуждений мультимодальных языковых моделей, в частности на русском языке, где не хватает диагностических датасетов для таких задач. Его цель — оценить способность моделей рассуждать о формах, цветах, количествах и пространственных отношениях в визуальных сценах, выходя за рамки базового понимания языка и проверяя способности моделей к комплексным рассуждениям. Данный навык необходим моделям, которые, как ожидается, будут анализировать визуальные данные и выполнять задачи, требующие логических выводов о взаимодействии объектов. Дизайн датасета, в котором используются структурированные семейства вопросов, обеспечивает всестороннюю и непредвзятую оценку, сосредоточенную на навыках рассуждения моделей, а не на распознавании паттернов.


## Описание датасета

### Поля данных

Каждый вопрос в датасете содержит следующие поля:

- `instruction` [str] — Промпт-инструкция для модели, содержащая шаблон для вставки элементов вопроса.
- `inputs` — Вводные данные, формирующие задание для модели. Могут включать одну или несколько модальностей - видео, аудио, изображение, текст.
    - `image` [str] — Путь к файлу с изображением, к которому относится вопрос.
    - `question` [str] — Текст вопроса.
- `outputs` [str] — Правильный ответ на вопрос.
- `meta` — Метаданные, относящиеся к тестовому примеру, но не используемые в вопросе (скрытые от тестируемой модели).
    - `id` [int] — Номер-идентификатор вопроса в датасете.
    - `question_type` [str] — Тип вопроса в зависимости от возможных ответов: бинарный, цвета, количество, материалы, формы, размер.
    - `image` — Метаданные, относящиеся к изображению.
        - `synt_source` [list] — Источники, с помощью которых сгенерированы или воссозданы данные для формирования вопроса, в том числе названия генеративных моделей.
        - `type` [str] — Тип изображения — согласно классификации изображений для датасетов MERA.


### Пример данных

```json
{
    "instruction": "Даны вопрос и картинка, необходимая для ответа на вопрос. Посмотри на изображение и дай ответ на вопрос. Ответом является одна цифра или одно слово в начальной форме.\nИзображение:<image>\nВопрос:{question}\nОтвет:",
    "inputs": {
        "image": "samples/image0123.png",
        "question": "Одинаков ли цвет большой металлической сферы и матового блока?"
    },
    "outputs": "да",
    "meta": {
        "id": 17,
        "question_type": "binary",
        "image": {
            "synt_source": [
                "blender"
            ],
            "type": "generated"
        }
    }
}
```


### Промпты

Для задачи были подготовлены 10 промптов, которые были равномерно распределены по вопросам по принципу "один вопрос – один промпт". Шаблоны в фигурных скобках в промпте заполняются из полей внутри поля `inputs` в каждом вопросе.


Пример промпта:

```
Даны вопрос и картинка, необходимая для ответа на вопрос. Посмотри на изображение и дай ответ на вопрос. Ответом является одна цифра или одно слово в начальной форме.
Изображение:<image>
Вопрос:{question}
Ответ:
```


### Создание датасета

Для создания RuCLEVR использовались два подхода: 1) генерация новых примеров и 2) аугментация данных с заменой цвета. Ниже каждый подход описан более подробно:

**Генерация новых примеров**: Были сгенерированы новые уникальные изображения и соответствующие вопросы с нуля. Этот процесс включал несколько этапов для обеспечения контролируемой и всесторонней оценки визуального рассуждения. Сначала автоматически генерировались 3D-изображения с использованием [Blender](https://www.blender.org/download/releases/2-78/) с изображением объектов с заданными свойствами, такими как форма, размер, цвет и материал. Эти объекты были размещены в различных конфигурациях для создания сложных сцен. Затем на основе заданных шаблонов были сгенерированы вопросы и ответы к ним. Чтобы избежать ошибок в грамматических формах (падежи, склонения), мы генерировали вопросы на английском языке, после чего перевели их на русский с помощью Google Translate. После генерации вопросы были отфильтрованы для выявления некорректных переводов с использованием модели [ruRoBERTa-large-rucola](https://huggingface.co/RussianNLP/ruRoBERTa-large-rucola), обученной для задачи лингвистической приемлемости. Кроме того, мы проверили датасет на отсутствие дубликатов.

**Аугментация данных с заменой цвета**: Нами были применены техники аугментации данных для повышения вариативности и сложности тестовой части с использованием разработанного [скрипта](https://github.com/erkenovaj/RuCLEVR/tree/main) для систематической замены цветов в вопросах и изображениях по заданным правилам. Аугментация изначально проводилась для сэмплов на английском языке, чтобы избежать морфологических сложностей. После аугментации вопросы были переведены на русский язык и проверены на грамматическую корректность.


## Оценка


### Метрики

Для агрегированной оценки ответов моделей используются следующие метрики:

- `Exact match`: Метрика Exact match вычисляет среднее по оценкам всех обработанных вопросов, где оценка имеет значение 1, если предсказанная строка точно совпадает с правильным ответом, и 0 в остальных случаях.
