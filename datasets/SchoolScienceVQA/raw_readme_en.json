{
    "Task description": "**{meta['dataset_name']}** is a Russian-language multimodal dataset inspired by [ScienceQA](https://scienceqa.github.io/index.html#home). It evaluates the reasoning capabilities of AI models in a multimodal setting using multiple-choice questions across scientific subjects such as physics, biology, chemistry, economics, history, and earth science. Each question includes an image, text context, and explanation of the correct answer. These components provide a basis for assessing reasoning chains.",
    "Motivation": "{meta['dataset_name']} is designed to benchmark AI systems in educational and scientific reasoning tasks requiring both visual and textual understanding. It supports the following use cases:\n\n- **Multimodal Model Evaluation**: The dataset requires joint processing of images and text. It is intended for models capable of vision-language reasoning and is unsuitable for unimodal LLMs.\n\n- **Target Audience**: Researchers and developers working on multimodal models, especially in the education and tutoring domain. Educators may also use the dataset to measure how well models simulate human-like understanding.\n\n- **Question Content**: Questions resemble real-world educational tasks and require true multimodal inference to solve correctly.",
    "Dataset creation": "{meta['dataset_name']} was developed from scratch based on the methodology of [ScienceQA](https://scienceqa.github.io/index.html#home), adapted for Russian cultural and educational context. Domains were adjusted to align with the Russian school curriculum.\n\nExpert annotators from relevant scientific domains created original multimodal examples. Images were produced using original photography, manual illustration, computer graphics, and neural network generation (DALLÂ·E, Stable Diffusion, etc.). All images are novel and not reused from existing datasets. Metadata includes image generation method to support transparency and bias mitigation.",
    "Contributors": "Maria Tikhonova, Yulia Lyakh",
    "Human baseline": "These tasks were presented to two groups: one consisting of untrained participants (overlap 5) and the other of domain experts (overlap 3). The aggregated answer was considered to be the one chosen by the majority (majority vote)."
}