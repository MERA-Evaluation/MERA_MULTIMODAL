{
    "Task description": "{meta[\"dataset_name\"]} is a multimodal emulation of the Turing test, which is an unchangeable sequence of question-answer tasks with the ability to choose an answer. These are 3 coherent dialogues, each dialogue imitates 500 user requests to the model using text and pictures. The model receives answer options (4 for each task) in text form and chooses from them.\n\nThe test tasks check the model's ability to adequately support a dialogue on naturally changing topics of communication, based on the context of previous questions.\n\nThe dataset is based on the text dataset of the same name from the first release of the MERA benchmark. In addition to {meta[\"dataset_name\"]}, a similar dataset is presented in 2 more versions: monomodal ruTiE-Txt and multimodal ruTie-Audio (questions are submitted to the input in audio format, the model responds with text).",
    "Motivation": "The dataset is designed to analyze models with a large context window (with a context depth of up to 499 questions), but can also be used for models with a smaller context window with a depth of 15 questions.\n\nThe test has a complex task. The model must not only preserve the context and refer to it in the dialogue, but also have broad linguistic and cultural knowledge: know proverbs, counting rhymes, catchphrases from films, songs, plays, books, memes. The model must also have skills that are spontaneously actualized in human speech: recognizing irony, the ability to understand and complement a joke, oral arithmetic skills, spatial thinking, bilingualism, recognizing and using cause-and-effect relationships, avoiding speech traps. Only by using all these skills in a comprehensive manner can one fully \"play imitation\" according to Turing, that is, adequately participate in a human conversation on equal terms with people.\n\nPlease note: in a conversation, the modalities of communication often change. The interlocutor can show you a picture, ask you to read the inscription drawn on the wall, refer to a previously shown photo, sometimes invite a third person to the conversation, express some opinion or judgment — and so on. Therefore, the design of a separate task in the {meta[\"dataset_name\"]} dialogue is not always designed as a question — it can be designed as a replica-sentence, to which the model needs to choose an adequate reaction. In {meta[\"dataset_name\"]}, a task can look like a simple picture sent to the model without an accompanying question — but with suggested reaction options, from which you need to choose the right one. The dataset offers 4 answer options for each question.\n\nThe test checks the model's ability\n- to retain context,\n- to support (at the everyday level) a dialogue on any of the main subject areas (see the MERA benchmark areas)\n- to understand the main classes of problems, without which it is impossible to solve the problems of emulating the Turing test (including the simplest mathematics, ethics, linguistic games, general worldview, etc.)\n- to navigate in various categories of thinking, including recognizing irony, emotions and intentions of the interlocutor, restoring the essence of the situation based on key elements, etc.\n\nThere is also an important limitation for the validity of checking models with ruTiE. Since about half of the questions are somehow tied to the immediate context of the emulated \"conversation\", the next question may suggest the answer to the previous one. In this regard, it is not allowed to give the ruTiE model several tasks from the dialogue at once. Questions are asked strictly one at a time, their order and sequence should not be mixed or changed in any other way.",
    "Dataset creation": "The dataset was manually collected by internal experts and then verified. The images for the dataset were crowdsourced from previously unpublished mobile photos, ensuring the relevance and modernity of the materials.",
    "Contributors": "Denis Shevelev, Artem Chervyakov, Alena Fenogenova, Sergey Markov (the author of the idea)",
    "Human baseline": "For all questions in the dataset, answers from annotators on a crowd-sourcing platform with an overlap of 5 were obtained. The aggregated answer was considered to be the one chosen by the majority (majority vote)."
}