# Оценка моделей на MERA

## Обзор <a name="head"></a>

Оценка моделей на бенчмарке MERA заключается в запуске процедуры генерации ответов модели на 
задачи бенчмарка. Генерация ответов осуществляется единообразно фреймворком 
[lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness).

Запуск оценки осуществляется из терминала (консоли). Команда для запуска формируется в зависимости 
от ряда факторов: какая модель будет оцениваться, какой фреймворк используется для инференса 
модели, содержат ли задачи для оценки на них модели ответы на вопросы тестовой части и т. п.

В обобщенном виде код запуска выглядит следующим образом:

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval <ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

Далее приводится инструкция по основным используемым значениям "переменных окружения" и 
"параметров запуска".

1. [Обзор](#head)
2. [Переменные окружения](#environs)
    1. [CUDA_VISIBLE_DEVICES](#cuda)
    2. [HuggingFace Переменные](#hf)
        1. [HF_HOME](#hf_home)
        2. [HF_TOKEN](#hf_token)
        3. [HF_DATASETS_IN_MEMORY_MAX_SIZE](#hf_ds_size)
    3. [TOKENIZERS_PARALLELISM](#token_parallel)
3. [Параметры запуска lm_eval](#lm_eval)
    1. [Общие Параметры](#general)
        1. [Задачи для оценки (--tasks)](#tasks)
        2. [Бэкенд для запуска модели (--model)](#model)
        3. [Устройство для расчетов (--device)](#device)
        4. [Количество фью-шотов (--num_fewshot)](#fewshots)
        5. [Размер батча (--batch_size, --max_batch_size)](#bs)
        6. [Сохранение логов замера (--log_samples, --output_path, --show_config)](#logs)
        7. [Ограничение размера датасета (--limit)](#limit)
        8. [Кэширование запросов и ответов модели (--use_cache, --cache_requests)](#cache)
        9. [Вывод нескольких запросов из датасета (--write_out)](#write)
        10. [Систем промпт (--system_instruction)](#system)
        11. [Чат темплейт и диалоговый режим (--apply_chat_template, --fewshot_as_multiturn)](#template)
        12. [Подача внешних задач (--include_path)](#incl_path)
        13. [Параметры генерации (--gen_kwargs)](#gen)
        14. [Детализация информации о запуске оценки модели (--verbosity)](#verbosity)
        15. [Режим генерации ответов без подсчета метрик (--predict_only)](#inference)
        16. [Управление случайностью (--seed)](#seed)
        17. [Дополнительные параметры](#additional)
    2. [Параметры модели (--model_args)](#model_args)
        1. [Transformers (--model hf)](#hf)
        2. [Transformers для мультимодальных моделей (--model hf-multimodal)](#hf_vlm)
        3. [vLLM (--model vllm)](#vllm)
        4. [vLLM для мультимодальных моделей (--model vllm-vlm)](#vllm_vlm)
        5. [API-модели](#api)
4. [Примеры запуска для разных типов моделей](#examples)
    1. [Transformers модель на текстовых задачах](#hf_text)
    2. [Transformers модель на мультимодальных задачах](#hf_mm)
    3. [OpenAI-like сервер (local-completions)](#local_compl)
    4. [vLLM на текстовых задачах](#vllm_ex)
    5. [vLLM на мультимодальной задаче](#vllm_mm)

## Переменные окружения <a name="environs"></a>

Переменные окружения задают общие "рамки" того, как код будет работать. Переменные окружения 
в большинстве случаев не оказывают влияния на саму процедуру оценки моделей (на генерацию ответов). 
Они нужны в первую очередь, чтобы задать конфигурацию среды, в которой замер модели будет 
осуществляться.

### CUDA_VISIBLE_DEVICES <a name="cuda"></a>

`CUDA_VISIBLE_DEVICES` позволяет вам указать, к каким видеокартам модуль `lm_eval` будет иметь 
"доступ". Если вы запускаете замер на ноде из 8 видеокарт, но при этом хотите, чтобы инференс 
модели осуществлялся на конкретной видеокарте, вы указываете номер данной видеокарты:

```bash
CUDA_VISIBLE_DEVICES=1 <ДРУГИЕ ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval <ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

Каждая видеокарта имеет свой номер (индекс). Увидеть индексы для видеокарт NVIDIA зачастую можно, 
если ввести в терминал команду:

```bash
nvidia-smi
```

Если данная команда поддерживается, вы получите на выход таблицу из всех ваших видеокарт с 
их номерами (индексами) и информацией об энергопотреблении, доступной видео памяти, запущенных 
процессах (с их id).

Если вы запускаете оценку модели на нескольких видеокартах, их номера пишутся через запятую без 
пробелов:

```bash
CUDA_VISIBLE_DEVICES=1,0,5 <ДРУГИЕ ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval <ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

Номера могут быть любыми из доступных. Они не обязаны быть последовательными или идти в 
возрастающем порядке. Если указанная вами видеокарта уже используется другой программой, то 
запуск может состояться, если хватит видеопамяти и на уже запущенную ранее программу, и на запуск 
оценки языковой модели через `lm_eval`.

Обращаем внимание, что `CUDA_VISIBLE_DEVICES` не гарантирует параллелизацию вычислений, а только 
позволяет ограничить круг видимых модулем `lm_eval` видеокарт. Параллелизация вычислений 
управляется внутри модуля `lm_eval` и использует те видеокарты, которые вы указали при запуске. 
Иначе говоря, если вы запустили:

```bash
CUDA_VISIBLE_DEVICES=0,1 <ДРУГИЕ ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval <ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

это не означает, что вычисления будут вестись параллельно на видеокартах с номерами `0` и `1`. Это 
означает только то, что код внутри `lm_eval` при обращении к видеокартам будет видеть данные две 
видеокарты и может запускать параллелизацию на них. Сама параллелизация управляется "параметрами 
запуска фреймворка".

### HuggingFace Переменные <a name="hf"></a>

Чаще всего датасеты для оценки моделей, а также сами модели загружаются с 
[HuggingFace](https://huggingface.co/). Существует ряд переменных окружения, которые позволяют 
контролировать процесс загрузки.

Более полный список переменных окружения, влияющих на работу с HuggingFace, доступен по 
[ссылке](https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables).

#### HF_HOME <a name="hf_home"></a>

`HF_HOME` указывает на директорию, где хранятся данные после загрузки моделей и датасетов с 
HuggingFace. Так, по пути `$HF_HOME/hub` хранятся кэш-файлы после скачивания датасетов/моделей. 
Альтернативно путь конкретно до директории с кэщ-файлами можно указать через переменную 
`HF_HUB_CACHE`.

```bash
HF_HOME="./cache" <ДРУГИЕ ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval <ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

Данная команда делает так, чтобы скачиваемые при запуске `lm_eval` модели и датасеты хранились по 
пути `./cache/hub`.

Параметр `HF_DATASETS_CACHE` указывает, что только кэш-файлы датасетов будут храниться по переданному 
пути. Так, вы можете хранить кэш-файлы датасетов в специальной директории, отличной от 
дефолтной.

#### HF_TOKEN <a name="hf_token"></a>

Переменная окружения `HF_TOKEN` заменяет собой токен для доступа к закрытым репозиториям на 
HuggingFace. Приватные датасеты, модели с ограниченным доступом требуют наличия токена, который 
выпущен аккаунтом, имеющим такой доступ. Например, если вы хотите загрузить модель 
`Ministral-8B-Instruct-2410`, вам придется получить сначала доступ к ней. Затем, когда доступ 
будет выдан вашему аккаунту, вы выпускаете [токен](https://huggingface.co/settings/tokens). 

Данный токен вы можете либо указать токеном по умолчанию, либо использовать непосредственно при 
запуске любого кода, который взаимодействует с HuggingFace.

Чтобы установить токен по умолчанию, необходимо ввести в терминале:

```bash
huggingface-cli login
```

В открывшемся меню вставляется ваш токен. Когда терминал напишет `Login successful`, переданный 
вами токен будет сохранен в файле по пути `$HF_HOME/token` (по умолчанию, можно переопределить, 
указав новый путь в переменной `HF_TOKEN_PATH`). Далее можно загружать модели и 
датасеты, не указывая каждый раз свой токен. 

Обратите внимание, что, если несколько человек работают на одной машине, они могут "затирать" 
токен друг друга в файле по указанному пути. Чтобы такого не происходило, можно либо указывать 
отдельную директорию в `HF_HOME`, либо использовать `HF_TOKEN` и каждый раз передавать корректный 
токен.

Чтобы передать токен через `HF_TOKEN`, введите токен, как в примере:

```bash
HF_TOKEN="XXX" <ДРУГИЕ ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval <ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

Вместо `XXX` подставляется сам токен. В таком случае модуль `lm_eval` будет видеть именно 
переданный вами токен при запуске. Обратите внимание, что действие `HF_TOKEN` носит "одноразовый" 
характер. Последующий запуск `lm_eval` без указания `HF_TOKEN` приведет к чтению токена из файла 
`$HF_HOME/token`. Потому, через `HF_TOKEN` токен передается при каждом запуске `lm_eval`.

#### HF_DATASETS_IN_MEMORY_MAX_SIZE <a name="hf_ds_size"></a>

По умолчанию датасеты с HuggingFace скачиваются в папку `$HF_HOME/hub` или `$HF_DATASETS_CACHE`. 
Далее файлы подгружаются из кэша (при обращении к датасету). Данный подход может оказаться 
довольно медленным. Альтернативно, можно загружать датасеты сразу в оперативную память, чтобы 
работа с датасетами была быстрее. 

Переменная `HF_DATASETS_IN_MEMORY_MAX_SIZE`cодержит число, которое означает, сколько байт 
оперативной памяти вы хотите выделить под хранение датасетов. Данное количество не должно 
превышать размер всей доступной оперативной памяти.

```bash
HF_DATASETS_IN_MEMORY_MAX_SIZE=23400000 <ДРУГИЕ ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval <ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

### TOKENIZERS_PARALLELISM <a name="token_parallel"></a>

При запуске оценки на некоторых моделях вы можете столкнуться с предупреждением вида:

```text
The current process just got forked. Disabling parallelism to avoid deadlocks... 
To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
```

Данная проблема возникает при взаимодействии параллельных вычислений в Python и Rust (если 
используется `FastTokenizer`, а часто он используется по умолчанию). Данная проблема может 
привести, как к просто предупреждению и не повлечь никаких последствий, так и повлечь остановку 
исполнения кода (может быть актуально для новых версий `PyTorch`).

Чтобы избежать возможных проблем, мы рекомендуем выключать параллелизацию в Python:

```bash
TOKENIZERS_PARALLELISM=false <ДРУГИЕ ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval <ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

## Параметры запуска lm_eval <a name="lm_eval"></a>

Все параметры запуска `lm_eval` описаны в `__main__.py` файле в репозитории модуля. Параметры 
передаются в linux-стиле через указание `-` или `--` перед самим названием параметра. Затем следует 
передаваемое значение (если требуется).

### Общие Параметры <a name="general"></a>

Общие параметры не зависят от конкретной модели или задачи и влияют в целом на процедуру замера.

#### Задачи для оценки (--tasks) <a name="tasks"></a>

Задачи для оценки модели передаются единым списком через запятую без пробелов. Допускается 
запуск оценки любого количества задач за один раз. Переданные задачи сначала обрабатываются по 
одной — для каждой задачи создается список всех запросов в модель. Затем все запросы сортируются 
по типу и помещаются в единые списки. Данные списки передаются в модель последовательно. 
Перед подачей в модель списки предварительно сортируются, потому не сохраняют структуру разбиения 
по отдельным задачам.

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --tasks gsm8k,hellaswag <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

#### Бэкенд для запуска модели (--model) <a name="model"></a>

**Обязательный параметр** `--model` может принимать ограниченный круг значений: 

- `hf` для инференса модели на основе библиотеки `transformers`
- `vllm` для инференса модели на основе библиотеки `vllm`
- `openai-completions` для запуска одной из не чатовых моделей OpenAI (babbage-002, davinci-002)
- `openai-chat-completions` для запуска одной из чатовых моделей OpenAI (gpt4o, gpt3.5, etc.)
- `local-completions` для обращения к не чатовым моделям, запущенным на OpenAI-like сервере (например, vllm.entrypoints.openai.api_server)
- `local-chat-completions` для обращения к чатовым моделям, запущенным на OpenAI-like сервере (например, vllm.entrypoints.openai.api_server)
- `nemo_lm` для инференса моделей на основе библиотеки `nemo`
- `mamba_ssm` для инференса моделей на основе библиотеки `mamba_ssm`
- `gguf` для инференса gguf моделей
- `anthropic-completions` для запуска одной из не чатовых моделей Anthropic
- `anthropic-chat-completions` для запуска одной из чатовых моделей Anthropic

Обращаем внимание, что под "не чатовыми" моделями понимаются такие модели, которые возвращают 
логиты для переданной на вход последовательности. Без такой особенности невозможно запускать 
`multiple_choice`, `loglikelihood`, `loglikelihood_rolling` задачи. "Чатовые" модели допускают 
запуск только `generate_until` задач. Тип каждой задачи может быть проверен в ее конфигурационном 
`yaml` файле. 

Выбранный вами бэкенд не означает, что выбранная вами модель обязательно будет на нем запущена. 
Некоторые модели, например, не реализованы в библиотеке `vllm`, потому при запуске оценки код 
упадет с ошибкой. Также выбранный вами бэкенд влияет напрямую на параметры `--model_args`.

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --model vllm <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

#### Устройство для расчетов (--device) <a name="device"></a>

**Необязательный параметр** `--device` содержит указание на то, какой тип вычислительного 
устройства будет использоваться для инференса модели:

- `cuda` для инференса моделей на видеокарте
- `cuda:0` для инференса моделей на видеокарте с номером 0 (не рекомендуется так передавать номер видеокарты, используйте `CUDA_VISIBLE_DEVICES`)
- `cpu` для инференса моделей на CPU (процессор, может быть крайне долго или вообще не запуститься для больших моделей)
- `mps` для инференса моделей на ARM процессорах Apple
- `xpu` для инференса моделей на Intel GPU (видеокартах от производителя Intel)

Возможны другие значения: проверяйте доступные в вашей версии `PyTorch` значения `device`.

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --device cuda <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

#### Количество фью-шотов (--num_fewshot) <a name="fewshots"></a>

Количество фью-шотов может управляться как из конфигурационных файлов задач, так и из терминала. 
Приоритет имеет терминал. То есть, если в yaml файле задачи установлено 1 фью-шот, а в терминале 
указано 2 фью-шота, то будет проводиться замер с 2 фью-шотами. Исключение составляет 0 фью-шотов. 
В некоторых версиях `lm_eval` наблюдалось поведение, при котором указание в yaml файле задачи 0 
фью-шотов полностью исключало какую-либо возможность изменения данного количества через терминал.

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --num_fewshot 2 <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

#### Размер батча (--batch_size, --max_batch_size) <a name="bs"></a>

Размер батча контролирует то, сколько запросов подается в модель за один раз. Запросы группируются 
и подаются батчами так, что пока один батч не закончил обрабатываться, другой не подается. Нарушается 
данное правило только при обращении к моделям по API — там доступен асинхронный режим замеров. 

Допускается указание не целого числа, а значения `auto`, при котором перед запуском процедуры оценки 
будет проведена процедура поиска оптимального размера батча (пробуются разные размеры, начиная 
с `--max_batch_size` в сторону уменьшения пока не перестанет выпадать Out-of-memory ошибка).

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --batch_size auto --max_batch_size 256 <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

#### Сохранение логов замера (--log_samples, --output_path, --show_config) <a name="logs"></a>

По умолчанию `lm_eval` не сохраняет запросы и ответы модели на них. Если вы включаете логирование 
запросов и ответов, они будут сохранены по пути `--output_path/model_name` в файлах вида 
`samples_dataset_name_*.jsonl`. Для каждой задачи сохраняется отдельный jsonl файл, содержащий
информацию о всех запросах, на которых производилась оценка модели: id запроса,
вид запроса в датасете (словарь прямо из датасета), текст запроса в модель 
(до truncation), ответ модели до пост-обработки, ответ модели после пост-обработки, хэш запроса.

Обращаем внимание, что метрики сохраняются отдельно по пути `--output_path/model_name` в файлах вида 
`results_*.json` вне зависимости от того, ставили ли вы флаг `log_samples`. Если вы запускаете
оценку больше, чем одной задачи за раз (в `--tasks` больше 
одного названия передано), в одной файле `results_*.json` будет сохранена основная информация обо 
всех задачах сразу. То есть, в отличие от файлов `samples_*.jsonl` (в которых один файл 
соответствует одной задаче), тут применяется группировка. Данные файлы содержат информацию о 
конфигурации задачи, метриках, а также деталях замера (использованный систем промпт, использованный 
чат темплейт, наличие включенного диалогового режима — multi-turn, время, потраченное на замер, 
параметры модели, хэш коммита репозитория, а также базовая информация о среде — версии основных 
библиотек, влияющих на замеры).

Вы можете самостоятельно выбрать, в какой директории будет создана папка, в которой будут сохранены 
лог-файлы по результатам замера (для одного замера один файл вида `results_*.json` и N файлов 
вида `samples_*.jsonl`, где N — число задач, на которых происходила оценка модели).

Обратите внимание, что результаты замера сохраняются не по пути `output_path`, а в директории внутри 
переданной. Имя директории будет соответствовать переданным `model_args`. Например, при указании:

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --model vllm \
--model_args pretrained=meta-llama/Llama-3.2-1B_vllm,dtype=bfloat16 \
--log_samples --output_path="$PWD/results/llama-3.2-1b" <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

Будет создана директория `results/Llama-3.2-1B_vllm/meta-llama__Llama-3.2-1B`, где будут лежать 
указанные файлы.

Обратите внимание, что название папки, в которой непосредственно хранятся логи, отражает только 
имя модели. То есть, при повторном запуске с другими параметрами, но тем же `output_path` логи 
будут сохранены в ту же папку `meta-llama__Llama-3.2-1B`.

Каждый лог-файл имеет в своем названии точную дату замера (завершения замера), потому старые 
файлы не будут затираться новыми.

Одному замеру соответствует один общий конфиг — описание самого замера из файла `results_*.json`. 
Данный конфиг по завершению замера может быть выведен в терминал, если вы укажете флаг 
`--show_config`. Вывод никак не влияет на сохранение данного конфига. 

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --model vllm \
--model_args pretrained=meta-llama/Llama-3.2-1B_vllm,dtype=bfloat16 \
--log_samples --output_path="$PWD/results/llama-3.2-1b" \
--show_config <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

#### Ограничение размера датасета (--limit) <a name="limit"></a>

Если вы по какой-то причине не хотите запускать целиком замер одной или нескольких задач (например, 
при дебаггинге), вы можете ограничить количество оцениваемых примеров из каждой задачи одинаковым 
числом N, которое не превосходит размер любой из оцениваемых задач. В таком случае, если задача 
подразумевает перемешивание вопросов, то сначала осуществляется перемешивание, затем берется 
подвыборка из N вопросов. И так для каждой задачи, на которых будет осуществлена оценка модели.

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --limit 10 <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

#### Кэширование запросов и ответов модели (--use_cache, --cache_requests) <a name="cache"></a>

Кэширование позволяет сохранять входные запросы и ответы моделей на них в отдельные файлы. Данные 
опции полезны, когда код падает с ошибкой. При перезапуске не нужно будет делать все генерации 
заново, а только те, что не были получены в первый раз.

Параметр `use_cache` передается с путем, по которому будет сохранен `db` файл вида 
`passed_name_rank0.db`. В базу данных будут сохранены запросы и ответы модели на них. Ключом 
является тип запроса и сам входной запрос. Иначе говоря, кэширование не различает разные модели, 
разные параметры запуска оценки. Если для двух разных моделей задать одинаковый путь в 
`use_cache` и запустить оценку на одинаковой задаче, то для второй модели загрузятся кэшированные 
ответы первой модели. Потому для каждого нового запуска следует указывать новое имя в `use_cache`.

При запуске:

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --use_cache ./lm_cache/llama-3.2-1b <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

Будет сохранен файл `lm_cache/llama-3.2-1b_rank0.db`. 

Допускается кэширование не только ответов моделей, но и самих входных запросов. Флаг `cache_requests` 
может находиться в трех значениях:
- `true`. При формировании списка запросов в модель они будут сохранены, чтобы при 
перезапуске замера не нужно было заново собирать тот же датасет, что может занимать значительное 
время
- `refresh`. Пересборка запросов и их кэширование
- `delete`. Удаление кэша запросов

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --cache_requests refresh <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

#### Вывод нескольких запросов из датасета (--write_out) <a name="write"></a>

При указании флага `write_out` несколько первых запросов из задачи выводятся в терминал, 
например, для валидации корректности работы сборки датасета.

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --write_out <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

#### Систем промпт (--system_instruction) <a name="system"></a>

Систем промпт передается один раз для всех задач, запускаемых одновременно. Систем промпт — это 
строка с текстом, который передается модели в качестве общего указания к стратегии поведения. В 
случае, если `apply_chat_template` отключен, `system_instruction` добавляется к каждому запросу 
слева (конкатенируется). Если `apply_chat_template` включен, систем промпт передается в виде 
отдельного словаря слева в списке:

```json
{
    "role": "system",
    "content": "Ты полезный ассистент, который следует инструкциям и не пишет в ответ ничего лишнего."
}
```

#### Чат темплейт и диалоговый режим (--apply_chat_template, --fewshot_as_multiturn) <a name="template"></a>

Современные инструктивные модели имеют в токенизаторе специальный параметр: `chat_template`. Это 
специальная функция (обычно в виде `jinja2` шаблона), которая особым образом формирует строку 
с запросов в языковую модель. При включении `apply_chat_template` изначально запрос формируется 
в виде словаря:

```json
{
    "role": "user",
    "content": "Реши пример: 2+2=4\n\nРеши задачу: 3+3=6\n\nРеши пример по математике: 2+3="
}
```

Иначе говоря, формируется обычный запрос, который кладется в словарь данного вида. При наличии 
`system_instruction` сначала в список со словарем добавляется словарь систем промпта. 
В итоге список словарей будет либо длины 1 (только запрос пользователя, либо 2 — систем промпт и запрос 
пользователя). Затем, данный список передается в chat_tempalte-функцию, превращающую по определенному 
правилу список словарей в строку вида, который "знаком" модели, с которым ее обучали 
разработчики.

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --apply_chat_template <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

Если `apply_chat_template` включен, то допускается включить диалоговый режим подачи запросов — 
multi-turn режим. В таком случае, если в задаче 0 фью-шотов, ничего не меняется. Однако, если 
фью-шотов больше нуля, каждый фью-шот переходит в пару словарей вида:

```Python
[
    {
        "role": "user",
        "content": "Реши пример: 2+2="
    },
    {
        "role": "assistant",
        "content": "4"
    }
]
```

Иначе говоря, `fewshot_as_multiturn` имитирует диалог пользователя с языковой моделью. Роль "user" — 
это запрос пользователя. Роль "assistant" — это ответ модели. Тестовый запрос подается в виде одного 
словаря с ролью "user" — только запрос, а модель на него должна ответить. Сформированный таким 
образом список из словарей передается в chat_tempalte-функцию и на выходе получается строка с 
запросом к модели.

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --apply_chat_template --fewshot_as_multiturn <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

#### Подача внешних задач (--include_path) <a name="incl_path"></a>

Внешними задачами мы будем считать любые задачи, которые не находятся по пути `lm_eval/tasks`. 
То есть, все задачи, которые не реализованы в `lm_eval` по умолчанию. Такие задачи лежат в 
отдельной папке и оформляются в стандартном виде: yaml файлы или py скрипты. Однако, в отличие от 
задач по пути `lm_eval/tasks`, они не добавлены в общий список задач, потому вызывать их нельзя.

Указание пути к папке с внешними задачами в параметре `include_path` позволяет добавить новые задачи 
в индекс и обращаться к ним:

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --include_path ./custom_tasks <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

#### Параметры генерации (--gen_kwargs) <a name="gen"></a>

Для всех переданных в `tasks` задач можно установить одинаковые параметры генерации (применимо, если 
задача типа `generate_until`, иначе игнорируется). Если одна или несколько вызванных задач имеют 
указанные в yaml файлах параметры генерации, то они игнорируются в пользу параметров, переданных 
в терминале. Если выбранная `model` не поддерживает указанный параметр, результат зависит от 
указанного типа бэкенда — может быть выпадение ошибки или игнорирование нечитаемых параметров.

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --gen_kwargs do_sample=true,temperature=0.6 <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

#### Детализация информации о запуске оценки модели (--verbosity) <a name="verbosity"></a>

Оценка языковой модели проходит через ряд определенных этапов, на которых ключевая информация 
фиксируется и может быть выведена в терминал. Поддерживается 5 уровней логирования процедуры 
оценки модели: `CRITICAL`, `ERROR`, `WARNING`, `INFO`, `DEBUG`. Каждый следующий уровень включает 
в себя все логи предыдущего уровня плюс новые. Выбор уровня логирования зависит от цели замера. 
Если замер делается только с целью получения метрик модели на задачах, разумно ставить уровни 
`CRITICAL`, `ERROR`, чтобы терминал не заполнялся большим количеством нерелевантной информации. 

Уровень `INFO` дает достаточно полное представление о ходе процедуры оценки. Уровень `DEBUG` дает 
наибольшее доступное количество логов для полного представления о ходе процедуры с информацией, 
которая необходима для дебаггинга замеров (при подозрении на наличие ошибок).

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --verbosity INFO <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

#### Режим генерации ответов без подсчета метрик (--predict_only) <a name="inference"></a>

В случае, если вам не нужны метрики модели или подсчет метрик недоступен, например, если у 
задачи закрытый тест (правильные ответы недоступны), рекомендуется включать флаг `predict_only`.
С данным параметром подсчет метрик не запускается. Любые метрики заменяются на `bypass`, а значения 
ставятся `999`. 

Если ответы на задания недоступны и параметр `predict_only` не включается пользователем для 
задач типа `multiple_choice` возможны многократные выводы в консоль предупреждения о том, что 
ответ не найден, так как данный тип задач подразумевает закрытое множество вариантов ответа, но 
правильный ответ (который отсутствует в датасете) в таком списке не указан. Параметр `predict_only` 
решает данную проблему. 

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --predict_only <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

#### Управление случайностью (--seed) <a name="seed"></a>

В `lm_eval` фиксируется 4 вида случайности:
- `python_random`
- `numpy_random`
- `torch_random`
- `fewshots_sampling_random`

Значения сидов передаются четырьмя значениями через запятую без пробела (в указанном порядке), 
либо одним значением, которое устанавливается одинаковым для четырех указанных случайностей.

Передача

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --seed 1,None,123,0 <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

Эквивалентно фиксации перед замером:

```python
random.seed(1)
torch.manual_seed(123)

# for fewshots sampling
random.seed(0)
```

#### Дополнительные параметры <a name="additional"></a>

`trust_remote_code` устанавливает одноименный флаг при загрузке датасетов и моделей, что разрешает 
исполнение внешнего кода:

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --trust_remote_code <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

`hf_hub_log_args` устанавливает параметры для загрузки результатов замера на HuggingFace.

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --hf_hub_log_args hub_results_org=EleutherAI,hub_repo_name=lm-eval-results <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

`wandb_args` управляет параметрами загрузки результатов замера на Wandb:

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --wandb_args project=lm-eval,job_type=eval <ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

### Параметры модели (--model_args) <a name="model_args"></a>

Параметры модели зависят от выбранного бэкенда (`--model`). Параметром `model_args` допускается 
передавать многие из параметров, которыми модель инициализируется в соответствующей библиотеке. 

Ниже будут рассмотрены параметры для 3 типов моделей.

#### Transformers (--model hf) <a name="hf"></a>

- `pretrained` — путь до модели: репозиторий или локальный путь, например, `Qwen/Qwen2-0.5B-Instruct` или `./my_saved_models/qwen`
Обращаем внимание, что путь до логов замера формируется из имени модели из `pretrained`. 
Если имя начинается с "." или "..", будет создана директория, которая будет скрытой и может 
не отображаться в визуальном интерфейсе файловой системы.

- `backend` — тип модели: `causal` (AutoModelForCausalLM) или `seq2seq` (AutoModelForSeq2seqLM)
- `tokenizer` — путь до токенизатора, при отсутствии используется токенизатор от `pretrained`
- `max_length` — размер контекста для замера (не должен превышать максимальный для данной модели)
- `dtype` — точность весов модели, передается в формате `bfloat16`, `float16`, и т.п.
- `add_bos_token` — использование специальных токенов (не только BOS), устанавливает `add_special_tokens=True`
- `parallelize` — логический флаг для разрешения использовать `accelerate` для оптимизации инференса на нескольких GPU
- `peft` — путь до репозитория или локальный путь до PEFT адаптера
- `delta` — путь до репозитория или локальный путь до Delta модели с весами, которые добавляются к весам `pretrained`
- `autogptq` — использовать класс `AutoGPTQForCausalLM`
- `gptqmodel` — использовать класс `GPTQModel`
- `attn_implementation` — какой модуль использовать для вычислений при инференсе, обычно либо отсутствует, либо `flash_attention_2`

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --model hf \
--model_args pretrained=Qwen/Qwen2-0.5B-Instruct,dtype=bfloat16,backend=causal,max_length=15000,add_bos_token=True,parallelize=True\
<ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

#### Transformers для мультимодальных моделей (--model hf-multimodal) <a name="hf_vlm"></a>

Те же параметры, что и для `--model hf`, плюс:
- `image_token_id` — ID токена, который кодирует `<image>`
- `max_images` — максимально допустимое для модели количество изображений в одном запросе
- `convert_img_format` — логическая переменная, нужно ли конвертировать изображение в RGB (img.convert("RGB"))

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> CUDA_VISIBLE_DEVICES=0,1 lm_eval --model hf-multimodal \
--model_args pretrained=Qwen/Qwen2-VL-2B-Instruct,dtype=bfloat16,parallelize=True,add_bos_token=True,convert_img_format=True\
<ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

#### vLLM (--model vllm) <a name="vllm"></a>

- `pretrained` — путь до модели: репозиторий или локальный путь, например, `Qwen/Qwen2-0.5B-Instruct` или `./my_saved_models/qwen`
Обращаем внимание, что путь до логов замера формируется из имени модели из `pretrained`. 
Если имя начинается с "." или "..", будет создана директория, которая будет скрытой и может 
не отображаться в визуальном интерфейсе файловой системы.
- `tensor_parallel_size` — количество видеокарт для параллелизации вычислений (инференса)
- `gpu_memory_utilization` — доля памяти, выделяемая для загрузки и инференса модели (на одной или нескольких видеокартах)
- `tokenizer` — путь до токенизатора, при отсутствии используется токенизатор от `pretrained`
- `add_bos_token` — использование специальных токенов (не только BOS), устанавливает `add_special_tokens=True`
- `max_length` — размер контекста для замера (не должен превышать максимальный для данной модели)
- `dtype` — точность весов модели, передается в формате `bfloat16`, `float16`, и т. п.
- `lora_local_path` — локальный путь до LoRa адаптера

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> CUDA_VISIBLE_DEVICES=0,1 lm_eval --model vllm \
--model_args pretrained=Qwen/Qwen2-0.5B-Instruct,dtype=bfloat16,tensor_parallel_size=2,max_length=15000,add_bos_token=True\
<ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

#### vLLM для мультимодальных моделей (--model vllm-vlm) <a name="vllm_vlm"></a>

Те же параметры, что и для `--model vllm`, плюс:
- `max_images` — максимально допустимое для модели количество изображений в одном запросе

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> CUDA_VISIBLE_DEVICES=0,1 lm_eval --model vllm \
--model_args pretrained=Qwen/Qwen2-VL-2B-Instruct,dtype=bfloat16,tensor_parallel_size=1,gpu_memory_utilization=0.5,add_bos_token=True\
<ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

#### API-модели <a name="api"></a>

На примере `local-completions`:

- `model` — путь до модели
- `base_url` — URL для отправки запросов, допустимо подставлять URL любого openai-like сервера, а также локального, например, `http://localhost:8000/v1/completions`
- `tokenizer_backend` — `tiktoken` | `huggingface` | `None`, бэкенд токенизатора. `tiktoken` — для токенизации запросов в OpenAI модели, `huggingface` — для HuggingFace моделей, None — в остальных случаях
- `tokenizer` — путь до токенизатора
- `tokenized_requests` — логическая переменная, нужно ли токенизировать запросы перед отправкой на сервер
- `num_concurrent` — количество параллельных запросов. Если > 1, включается асинхронный режим с указанным количеством запросов в качестве лимита по одновременно отправленным на сервер
- `seed` — зерно случайности
- `add_bos_token` — использование специальных токенов (не только BOS), устанавливает `add_special_tokens=True`
- `verify_certificate` — логическая переменная, нужно ли подтверждать сертификаты
- `timeout` — значение таймаута для ожидания ответа на один запрос в секундах
- `max_retries` — количество попыток отправить запрос заново, если возникла ошибка

```bash
<ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ> lm_eval --model local-completions \
--model_args model=Qwen/Qwen2.5-7B-Instruct,num_concurrent=128,base_url=http://localhost:7996/v1/completions,tokenizer_backend=huggingface,tokenizer=Qwen/Qwen2.5-7B-Instruct,timeout=1000000,add_bos_token=True\
<ДРУГИЕ ПАРАМЕТРЫ ЗАПУСКА ФРЕЙМВОРКА>
```

## Примеры запуска для разных типов моделей <a name="examples"></a>

### Transformers модель на текстовых задачах <a name="hf_text"></a>

```bash
CUDA_VISIBLE_DEVICES=2,3 HF_DATASETS_CACHE="./ds_cache" TOKENIZERS_PARALLELISM=false \
HF_DATASETS_IN_MEMORY_MAX_SIZE=23400000 PYTHONPATH=$PWD lm_eval --model hf \
--model_args pretrained=google/gemma-2-2b-it,dtype=bfloat16,add_bos_token=True,parallelize=True \
--output_path="$PWD/mera_results/gemma-2-2b-it" \
--include_path=./benchmark_tasks --use_cache lm_cache/gemma-2-2b-it \
--device cuda --batch_size=1 --predict_only --log_samples --seed 1234 --verbosity ERROR \
--apply_chat_template --trust_remote_code \
--tasks rummlu,mamuramu,rummlu_gen,mamuramu_gen
```

### Transformers модель на мультимодальных задачах <a name="hf_mm"></a>

```bash
CUDA_VISIBLE_DEVICES=0 lm_eval --model hf-multimodal \
--model_args pretrained=Qwen/Qwen2-VL-2B-Instruct,attn_implementation=flash_attention_2,dtype=bfloat16,convert_img_format=True \
--device cuda --output_path="$PWD/multimodal_results" --batch_size=1 --predict_only \
--log_samples --seed 1234 --num_fewshot=0 --apply_chat_template --fewshot_as_multiturn \
--include_path ./multimodal_tasks --tasks ruclevr,,ruvqa --limit 3
```

### OpenAI-like сервер (local-completions) <a name="local_compl"></a>

```bash
TOKENIZERS_PARALLELISM=false PYTHONPATH=$PWD lm_eval --model local-completions \
--model_args model=meta-llama/Llama-3.2-3B-Instruct,num_concurrent=128,base_url=http://localhost:8000/v1/completions,tokenizer_backend=huggingface,tokenizer=meta-llama/Llama-3.2-3B-Instruct,timeout=1000000 \
--batch_size=1 --predict_only --log_samples --seed 1234 --verbosity ERROR --num_fewshot=0 \
--output_path="./results/Llama-3.2-3B-Instruct" --use_cache="cache/llama-32-3b-instruct" \
--include_path=./benchmark_tasks --tasks simplear,bps
```

### vLLM на текстовых задачах <a name="vllm_ex"></a>

```bash
CUDA_VISIBLE_DEVICES=0,1 HF_DATASETS_CACHE="./ds_cache" TOKENIZERS_PARALLELISM=false \
HF_DATASETS_IN_MEMORY_MAX_SIZE=23400000 PYTHONPATH=$PWD lm_eval --model vllm \
--model_args pretrained=../models/Local_Qwen,dtype=bfloat16,add_bos_token=True,tensor_parallel_size=2,gpu_memory_utilization=0.7 \
--output_path="$PWD/results/Local_Qwen" --include_path=./benchmark_tasks \
--use_cache lm_cache/Local_Qwen --device cuda --batch_size=1 --predict_only \
--log_samples --seed 1234 --verbosity ERROR --trust_remote_code \
--tasks simplear,bps_gen
```

### vLLM на мультимодальной задаче <a name="vllm_mm"></a>

```bash
CUDA_VISIBLE_DEVICES=7 lm_eval --model vllm-vlm \
--model_args pretrained=Qwen/Qwen2-VL-2B-Instruct,dtype=bfloat16,tensor_parallel_size=1,gpu_memory_utilization=0.5 \
--device cuda --output_path="$PWD/test_ruvqa" --batch_size=1 \
--log_samples --seed 1234 --num_fewshot=0 --apply_chat_template --fewshot_as_multiturn \
--include_path ./multimodal_tasks --tasks ruvqa
```
























