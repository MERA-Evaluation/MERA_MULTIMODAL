# Human Baseline

После того, как датасет прошел валидацию организаторами, и его версия в нужном формате зафиксирована, и подготовлен код для запуска оценки на задаче, — можно провести оценку уровня человека на этом датасета (human baseline).


## Общие правила

**Зачем делать human baseline?**  Мы уже умеем замерять модели, но нам важно понимать, насколько эта задача сложна для человека.

Мы выделяем два уровня сложности датасетов:
- **Базовый тест** — для решения которого достаточно знаний школьной программы, может решить практически любой взрослый человек. В списке тестируемых [навыков](skills_tax.md) из категории "Knowledge": `Common everyday knowledge` и/или `Common domain knowledge`.
- **Экспертный тест** — для ответа на который нужны специальные знания и подготовка. Для экспертных датасетов необходимо провести две оценки: с профессиональными экспертами и с людьми без специальной подготовки. В списке тестируемых [навыков](skills_tax.md) из категории "Knowledge": `Expert domain knowledge`. Для экспертных тестов нужно провести human baseline отдельно с экспертами и с участниками без профессиональной подготовки.


### Как сделать инструкцию

Человек и машина должны оказаться в максимально похожих условиях при проведении теста. Инструкции у человека и машины, очевидно, разные: например, на краудсорсинговой платформе — это инструкция и проект обучения, чтобы человек понял формат работы и что нужно делать; у машины — это данные в SFT и/или фью-шоты при прогоне.

В инструкцию по разметке и проект необходимо **добавить все ограничения**, которые могут повлиять на человеческую оценку (например, запретить гуглить, пользоваться калькуляторами или использовать нейросети для решения). Последнее особенно важно для приватных датасетов, иначе вопросы утекут в модели, которые могут в дальнейшем оцениваться на бенчмарке.

Важно написать подробную инструкцию по разметке датасета для разметчиков (в Толоке/Яндекс.Заданиях/ABC Elementary/TagME), обязательно включить туда несколько примеров на все основные репрезентативные случаи. Например, если в датасете разные типы ответов и разные домены, включите примеры из разных доменов с разными типами ответов. Это достаточный фью-шот для человека.


### Формулировка заданий

Человек при оценке должен находится в условиях, максимально приближенных к тем, в которых находится модель (машина) при прогоне. Необходимо соблюдать единый формат текста как для человека, так и для модели (машины).

**Задание и вопрос теста**. Если в модель вы подаете картинку и 4 варианта ответа, то и человеку в проекте вы подаете на вход картинку и 4 варианта ответа. Просите машину выбрать букву из ABCD — в таком случае просите человека написать букву (а не писать ответ текстом) или выбирать ответ из предложенных галочкой / интерфейсом.
В интерфейсе задания фью-шоты писать не нужно.

Пример инструкции для разметчиков human baseline для датасета [BPS в MERA v1.0.0](https://mera.a-ai.ru/ru/tasks/20) можно изучить [по ссылке](https://docs.google.com/document/d/1fkPhS0UCDNOLZ6uOCprHsIZQrADCWuwbAN35M_v6BWs/edit?tab=t.0).


### Отбор разметчиков

Все тестирования human baseline (HB) нужно делать **с перекрытием (overlap) разметчиков = 5, на средне-статистических людях**.
Это значит, что это не специально обученные ИИ-тренеры, а **краудсорс** на любой удобной платформе (например, в Толоке, Яндекс.Заданиях, ABC Elementary или TagME). Ограничений на количество примеров на странице разметки нет.


### Контрольные вопросы (honeypots)

Если возможно для датасета, закладываем в пул разметки **honeypots** — контрольные вопросы для оценки способности разметчика решать задачу (https://ru.wikipedia.org/wiki/Honeypot). Это поможет не учитывать при агрегации ответы пользователей, которые не поняли задание или хакают проект. 

Хорошей практикой является делать 5% от всего датасета контрольными вопросами. Для контрольного вопроса ответ известен заранее. Контрольные вопросы регулярно вставляются в задание на разметку, но не повторяются. По ответам на эти вопросы можно судить о “навыке” разметчика — насколько хорошо и добросовестно он справляется с разметкой. Контрольные вопросы должны быть максимально похожи на вопросы датасета, чтобы разметчик не мог с ходу отличить одни от других.


### Human baseline для больших датасетов

Если датасет объемный (>3000 вопросов), и провести для него человеческую оценку целиком затруднительно, можно провести ее на репрезентативном подмножестве вопросов. Нет смысла прогонять человеческую разметку на десятках тысячах примерах. Мы держим баланс соблюдения методологии и адекватного расходования ресурсов и времени. Например, при исходном датасете объемом в 10000 примеров, для human baseline можно выделить сабсет из 1000 примеров, сбалансированных так же, как исходный датасет. 

❗️Обратите внимание, подвыборка должна совпадать с оригинальным датасетом по распределению доменов/классов, а также всех остальных признаков, для которых есть такая возможность.❗️


## Агрегация данных разметки

По результатам разметки информация должна быть агрегирована. Для отчета о проведенном human baseline нужны следующие расчеты.

### Согласие аннотаторов

Это может быть классический Dawid Skene или Majority Vote (MV; ответ, который выбрало большинство аннотаторов — из 5 разметчиков выбран как минимум 3-мя). В качестве справки можно почитать [статью](https://toloka.ai/docs/guide/result-aggregation/).

### Доля примеров с низкой согласованностью

Порог согласованности определяется в зависимости от задачи, при выборе порога рекомендуем ориентироваться на аналогичные статьи и/или датасеты. Если доля согласованных примеров выше заданного порога, в дальнейших расчетах учитываются только согласованные примеры.

Если доля несогласованных ответов выше порогового значения, то оценка является некорректной, использовать ее нельзя. Возможно, инструкции в проекте были недостаточно точными, или в датасете есть ошибки. Иногда датасет специфичен и требует специальных навыков и знаний (например, определение длины максимальной общей подстроки, поиск ошибок в коде, решение сложных математических примеров).


### Агрегированный ответ на каждый вопрос

Ответы должны быть определены для каждого вопроса в тесте. В последней итерации не должно быть "выброшенных" сэмплов. Если есть сэмпл, где MV не дал результатов, можно брать не простое голосование, а взвешивать голоса в зависимости от “навыка” разметчика на контрольных заданиях (предпочтение отдается более точным разметчикам, которые дали большее количество правильных ответов). Если точности на контрольных заданиях недостаточно, то можно рассчитать “навыки” разметчиков с помощью [GLAD](https://toloka.ai/docs/crowd-kit/reference/crowdkit.aggregation.classification.glad.GLAD/) (EM-алгоритм).


### Качество ответов разметчиков

Проверяем, нет ли “плохих” разметчиков. Выберите порог точности на контрольных заданиях и по этому порогу принимайте решение, достаточно ли хорошо справился разметчик. Если по проекту мы видим, что конкретный разметчик блокируется по контрольным вопросам и портит согласие аннотаторов, его ответы нужно исключить из выборки.


### Ошибки в датасете

Если отдельные примеры не согласованы (нет согласия между разметчиками), и ошибка разметчика исключена (по остальным датасетам он отвечает согласованно с остальными), то это повод подозревать, что ошибка в датасете. В этом случае нужно пересмотреть и исправить датасет. Понять, связаны ли ошибки с человеческим фактором, можно только после анализа ошибок. Например, в задачах на решение математических примеров часто встречаются ошибки на 10, 100, 1000 и так далее. Это обусловлено тем, что люди ошибаются в одной цифре либо в подсчетах в уме/в столбик, либо, вбивая в калькулятор пример. Такая ошибка не означает, что датасет плохой.


### Общее качество ответов

По согласованным примерам нужно посчитать общий скор относительно золотых ответов (правильных). Эту финальную метрику репортим [в метаданных датасета](dataset_formatting.md#метаданные-датасета-raw_dataset_metajson) (поле `human_benchmark`). Вычислять нужно ту метрику, которая заявлена для данного датасета (там же в поле `metrics`).


## Оценка HB для экспертных тестов

Для датасетов экспертного уровня сложности нужно провести два этапа Human Baseline: с экспертами и с людьми без специальной подготовки. Методология проведения HB с неспециалистами описана выше, особенности проведения экспертной оценки описаны далее.

Если у вас есть эксперты, которые еще не видели вопросы датасета, то экспертный HB проводится по базовой инструкции выше, только вместо крауд-разметчиков участвуют эксперты.

Если участвовать в экспертном HB будут те же люди, которые создавали вопросы датасеты, то важно обеспечить честную оценку. Для этого следуем инструкции ниже.

**Алгоритм оценки (по аналогии с OOB-оценкой из бэггинга)**.
Алгоритм, при котором каждый эксперт (та же группа экспертов, которые делали датасет, если нет возможности найти других) отвечает на вопросы, пропуская те, которые были придуманы им самим. Это позволяет получить объективные оценки, избегая предвзятости.

Рассмотрим для примера некоторый экспертный датасет с 5 доменами.

Этапы проведения оценки:

1. Отбор вопросов:
    - Для каждого домена выбираем сэмпл из N вопросов (например, 100, итого 500 вопросов).
    - Вопросы отбираются равномерно по субдоменам и уровню сложности.

2. Разметка:
    - Эксперты домена отвечают на все вопросы, кроме тех, которые эксперт разметил сам.
    - Для исключения собственных вопросов, для каждого вопроса добавляем опцию "пропустить вопрос, так как он, возможно, был придуман мной".
    - Эксперты обязаны воспользоваться этой опцией, если у них возникают сомнения или подозрения, что они являются авторами вопроса.

3. Ограничение по времени (опционально):
    - Эксперт должен ответить на вопрос в течение одной минуты с момента подгрузки вопроса, чтобы исключить возможность поиска ответа в интернете или долгих раздумий. Это обеспечивает более объективный human baseline и ускоряет разметку.

Итог:
- Каждый вопрос получает N-k ответ, где N — количество экспертов в домене, k — это число экспертов, который отметили вопрос как потенциально вопрос их авторства.
- Агрегированный ответ выбирается по методу majority vote (ответ, который выбрало большинство экспертов, ответивших на вопрос без сомнения об авторстве).

Оценка времени:
- Средняя скорость оценки: 5 вопросов за 10 минут (включая чтение вопроса, контекста, просмотр картинки и принятие решения).
- Производительность: 30 вопросов в час.
- Время на разметку: 3-4 часа для выполнения полного набора вопросов (100 штук - свои вопросы).

Этот подход позволяет создать наиболее объективный Human Baseline для каждого домена, ускорить процесс оценки и минимизировать влияние предвзятости экспертов.


## Отчет о проведенном HB (definition of done)

- ✅ Методология проведения HB описана на русском и английском языках в [карточках](dataset_formatting.md#текстовые-описания-raw_readme_rumdи-raw_readme_enmd) в разделе Human baseline (в репозитории), с указанием всех деталей, которые упомянуты в этой инструкции. Значение агрегированных метрик HB внесено [в метаданные датасета](dataset_formatting.md#метаданные-датасета-raw_dataset_metajson).
- ✅ Файлы с исходными и агрегированными оценками и воспроизводимый код для агрегации загружены в репозиторий в папке соответствующего датасета в подпапку `hb/`. Код можно выложить в виде jupyter-ноутбука, в таком случае он должен вопроизводиться без ошибок через Run All.
- ✅ Информация по затратам (по базовому и экспертному HB отдельно, если экспертный проводился) и методологии HB отправлена на почту mera@a-ai.ru. В отчет нужно включить общие затраты на весь проект и почасовая ставка оплаты труда разметчиков. Большинство платформ дают возможность понять, сколько времени разметчики тратили на разметку сэмплов. Необходимо взять эту информацию и рассчитать, какова была почасовая оплата труда разметчиков.
